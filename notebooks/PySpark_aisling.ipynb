{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook2\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"1g\").\\\n",
    "        config(\"spark.mongodb.input.uri\",\"mongodb://mongo1:27017,mongo2:27018,mongo3:27019/database.horses_collection?replicaSet=rs0\").\\\n",
    "        config(\"spark.mongodb.output.uri\",\"mongodb://mongo1:27017,mongo2:27018,mongo3:27019/database.horses_collection?replicaSet=rs0\").\\\n",
    "        config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading dataframes from MongoDB\n",
    "df = spark.read.format(\"mongo\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- clk: string (nullable = true)\n",
      " |-- mc: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- marketDefinition: struct (nullable = true)\n",
      " |    |    |    |-- betDelay: integer (nullable = true)\n",
      " |    |    |    |-- bettingType: string (nullable = true)\n",
      " |    |    |    |-- bspMarket: boolean (nullable = true)\n",
      " |    |    |    |-- bspReconciled: boolean (nullable = true)\n",
      " |    |    |    |-- complete: boolean (nullable = true)\n",
      " |    |    |    |-- countryCode: string (nullable = true)\n",
      " |    |    |    |-- crossMatching: boolean (nullable = true)\n",
      " |    |    |    |-- discountAllowed: boolean (nullable = true)\n",
      " |    |    |    |-- eachWayDivisor: double (nullable = true)\n",
      " |    |    |    |-- eventId: string (nullable = true)\n",
      " |    |    |    |-- eventName: string (nullable = true)\n",
      " |    |    |    |-- eventTypeId: string (nullable = true)\n",
      " |    |    |    |-- inPlay: boolean (nullable = true)\n",
      " |    |    |    |-- marketBaseRate: double (nullable = true)\n",
      " |    |    |    |-- marketTime: string (nullable = true)\n",
      " |    |    |    |-- marketType: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- numberOfActiveRunners: integer (nullable = true)\n",
      " |    |    |    |-- numberOfWinners: integer (nullable = true)\n",
      " |    |    |    |-- openDate: string (nullable = true)\n",
      " |    |    |    |-- persistenceEnabled: boolean (nullable = true)\n",
      " |    |    |    |-- regulators: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- runners: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- adjustmentFactor: double (nullable = true)\n",
      " |    |    |    |    |    |-- bsp: string (nullable = true)\n",
      " |    |    |    |    |    |-- id: integer (nullable = true)\n",
      " |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |-- removalDate: string (nullable = true)\n",
      " |    |    |    |    |    |-- sortPriority: integer (nullable = true)\n",
      " |    |    |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- runnersVoidable: boolean (nullable = true)\n",
      " |    |    |    |-- settledTime: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- suspendTime: string (nullable = true)\n",
      " |    |    |    |-- timezone: string (nullable = true)\n",
      " |    |    |    |-- turnInPlayEnabled: boolean (nullable = true)\n",
      " |    |    |    |-- venue: string (nullable = true)\n",
      " |    |    |    |-- version: integer (nullable = true)\n",
      " |    |    |-- rc: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- ltp: double (nullable = true)\n",
      " |    |    |    |    |-- id: integer (nullable = true)\n",
      " |-- op: string (nullable = true)\n",
      " |-- pt: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_exploded = df.select('*', explode(df.mc).alias('mc_row'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+---+-------------+--------------------+\n",
      "|                 _id|       clk|                  mc| op|           pt|              mc_row|\n",
      "+--------------------+----------+--------------------+---+-------------+--------------------+\n",
      "|[619e9df0677066b2...|2482662312|[[1.124690101,, [...|mcm|1462562564947|[1.124690101,, [[...|\n",
      "|[619e9df0677066b2...|2482662312|[[1.124690101,, [...|mcm|1462562564947|[1.124690102,, [[...|\n",
      "|[619e9df0677066b2...|2482662312|[[1.124690101,, [...|mcm|1462562564947|[1.124690103,, [[...|\n",
      "|[619e9df0677066b2...|2482669649|[[1.124690101,, [...|mcm|1462562624985|[1.124690101,, [[...|\n",
      "|[619e9df0677066b2...|2482669649|[[1.124690101,, [...|mcm|1462562624985|[1.124690103,, [[...|\n",
      "|[619e9df0677066b2...|2482677346|[[1.124690101,, [...|mcm|1462562684974|[1.124690101,, [[...|\n",
      "|[619e9df0677066b2...|2482677346|[[1.124690101,, [...|mcm|1462562684974|[1.124690102,, [[...|\n",
      "|[619e9df0677066b2...|2482677346|[[1.124690101,, [...|mcm|1462562684974|[1.124690103,, [[...|\n",
      "|[619e9df0677066b2...|2482684635|[[1.124690101,, [...|mcm|1462562744944|[1.124690101,, [[...|\n",
      "|[619e9df0677066b2...|2482684635|[[1.124690101,, [...|mcm|1462562744944|[1.124690102,, [[...|\n",
      "+--------------------+----------+--------------------+---+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mc_exploded.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mc_exploded = mc_exploded.limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#md_only = mc_exploded.filter(mc_exploded.mc_row.marketDefinition.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#md_only = md_only.where(market_definitions.marketType == \"WIN\") #.collect()\n",
    "\n",
    "md_only = mc_exploded.filter(mc_exploded.mc_row.marketDefinition.isNotNull()) and mc_exploded.where(mc_exploded.mc_row.marketDefinition.marketType == \"WIN\") and mc_exploded.where(mc_exploded.mc_row.marketDefinition.countryCode == \"GB\") and mc_exploded.where(mc_exploded.mc_row.id == \"1.124691542\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_only = mc_exploded.filter(mc_exploded.mc_row.marketDefinition.isNotNull()) and mc_exploded.where(mc_exploded.mc_row.id == \"1.124691542\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the marketDefinition array.\n",
    "market_definitions = md_only.selectExpr('op AS operation_type',\n",
    "                                        'clk AS sequence_token',\n",
    "                                        'pt AS published_time',\n",
    "                                        'mc_row.id AS market_id',\n",
    "                                        'mc_row.rc AS rc',\n",
    "                                        'mc_row.marketDefinition.betDelay AS bet_delay',\n",
    "                                        'mc_row.marketDefinition.bettingType AS betting_type',\n",
    "                                        'mc_row.marketDefinition.bspMarket AS bsp_market',\n",
    "                                        'mc_row.marketDefinition.bspReconciled AS bsp_reconciled',\n",
    "                                        'mc_row.marketDefinition.complete AS complete',\n",
    "                                        'mc_row.marketDefinition.countryCode AS country_code',\n",
    "                                        'mc_row.marketDefinition.crossMatching AS cross_matching',\n",
    "                                        'mc_row.marketDefinition.discountAllowed AS discount_allowed',\n",
    "                                        'mc_row.marketDefinition.eventId AS event_id',\n",
    "                                        'mc_row.marketDefinition.eventName AS event_name',\n",
    "                                        'mc_row.marketDefinition.eventTypeId AS event_type_id',\n",
    "                                        'mc_row.marketDefinition.inPlay AS in_play',\n",
    "                                        'mc_row.marketDefinition.marketBaseRate AS market_base_rate',\n",
    "                                        'mc_row.marketDefinition.marketTime AS market_time',\n",
    "                                        'mc_row.marketDefinition.marketType AS market_type',\n",
    "                                        'mc_row.marketDefinition.numberOfActiveRunners AS number_of_active_runners',\n",
    "                                        'mc_row.marketDefinition.numberOfWinners AS number_of_winners',\n",
    "                                        'mc_row.marketDefinition.openDate AS open_date',\n",
    "                                        'mc_row.marketDefinition.persistenceEnabled AS persistence_enabled',\n",
    "                                        'mc_row.marketDefinition.runnersVoidable AS runners_voidable',\n",
    "                                        'mc_row.marketDefinition.settledTime AS settled_time',\n",
    "                                        'mc_row.marketDefinition.status AS status',\n",
    "                                        'mc_row.marketDefinition.suspendTime AS suspend_time',\n",
    "                                        'mc_row.marketDefinition.timezone AS timezone',\n",
    "                                        'mc_row.marketDefinition.turnInPlayEnabled AS turn_in_play_enabled',\n",
    "                                        'mc_row.marketDefinition.version AS version',\n",
    "                                        'mc_row.marketDefinition.name AS market_name',\n",
    "                                        'mc_row.marketDefinition.regulators AS regulators',\n",
    "                                        'mc_row.marketDefinition.runners AS runners')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#market_definitions = market_definitions.where(market_definitions.country_code == \"GB\") #.collect()\n",
    "\n",
    "#market_definitions = market_definitions.where(market_definitions.market_type == \"WIN\") #.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#market_definitions = market_definitions.where(market_definitions.market_id == \"1.124664295\") #.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#market_definitions = market_definitions.limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(operation_type='mcm', sequence_token='2461683065', published_time=1462526770248, market_id='1.124691542', rc=None, bet_delay=0, betting_type='ODDS', bsp_market=False, bsp_reconciled=False, complete=True, country_code='GB', cross_matching=False, discount_allowed=True, event_id='27786741', event_name='Ling (RFC) 6th May', event_type_id='7', in_play=False, market_base_rate=5.0, market_time='2016-05-06T12:30:00.000Z', market_type='REV_FORECAST', number_of_active_runners=36, number_of_winners=1, open_date='2016-05-06T12:30:00.000Z', persistence_enabled=True, runners_voidable=False, settled_time=None, status='OPEN', suspend_time='2016-05-06T12:30:00.000Z', timezone='Europe/London', turn_in_play_enabled=False, version=1308185913, market_name='Reverse FC', regulators=['MR_INT'], runners=[Row(adjustmentFactor=None, bsp=None, id=2521889, name='1 - 2 / 2 - 1', removalDate=None, sortPriority=1, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521890, name='1 - 3 / 3 - 1', removalDate=None, sortPriority=2, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521892, name='1 - 4 / 4 - 1', removalDate=None, sortPriority=3, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521899, name='1 - 6 / 6 - 1', removalDate=None, sortPriority=4, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521904, name='1 - 7 / 7 - 1', removalDate=None, sortPriority=5, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521913, name='1 - 8 / 8 - 1', removalDate=None, sortPriority=6, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522761, name='1 - 9 / 9 - 1', removalDate=None, sortPriority=7, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648359, name='1- 10 / 10 - 1', removalDate=None, sortPriority=8, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521891, name='2 - 3 / 3 - 2', removalDate=None, sortPriority=9, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521893, name='2 - 4 / 4 - 2', removalDate=None, sortPriority=10, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521900, name='2 - 6 / 6 - 2', removalDate=None, sortPriority=11, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521905, name='2 - 7 / 7 - 2', removalDate=None, sortPriority=12, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521914, name='2 - 8 / 8 - 2', removalDate=None, sortPriority=13, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522762, name='2 - 9 / 9 - 2', removalDate=None, sortPriority=14, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648362, name='2 - 10 / 10 - 2', removalDate=None, sortPriority=15, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521894, name='3 - 4 / 4 - 3', removalDate=None, sortPriority=16, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521901, name='3 - 6 / 6 - 3', removalDate=None, sortPriority=17, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521908, name='3 - 7 / 7 - 3', removalDate=None, sortPriority=18, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521915, name='3 - 8 / 8 - 3', removalDate=None, sortPriority=19, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522763, name='3 - 9 / 9 - 3', removalDate=None, sortPriority=20, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648363, name='3 - 10 / 10 - 3', removalDate=None, sortPriority=21, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521902, name='4 - 6 / 6 - 4', removalDate=None, sortPriority=22, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521909, name='4 - 7 / 7 - 4', removalDate=None, sortPriority=23, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521916, name='4 - 8 / 8 - 4', removalDate=None, sortPriority=24, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522764, name='4 - 9 / 9 - 4', removalDate=None, sortPriority=25, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648364, name='4 - 10 / 10 - 4', removalDate=None, sortPriority=26, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521911, name='6 - 7 / 7 - 6', removalDate=None, sortPriority=27, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521918, name='6 - 8 / 8 - 6', removalDate=None, sortPriority=28, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522766, name='6 - 9 / 9 - 6', removalDate=None, sortPriority=29, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648368, name='6 - 10 / 10 - 6', removalDate=None, sortPriority=30, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521919, name='7 - 8 / 8 - 7', removalDate=None, sortPriority=31, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522768, name='7 - 9 / 9 - 7', removalDate=None, sortPriority=32, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648371, name='7 - 10 / 10 - 7', removalDate=None, sortPriority=33, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522769, name='8 - 9 / 9 - 8', removalDate=None, sortPriority=34, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648372, name='8 - 10 / 10 - 8', removalDate=None, sortPriority=35, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648374, name='9 - 10 / 10 - 9', removalDate=None, sortPriority=36, status='ACTIVE')])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "market_definitions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to only the records that have Runners.\n",
    "runners_only = market_definitions.filter(market_definitions.runners.isNotNull()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(operation_type='mcm', sequence_token='2461683065', published_time=1462526770248, market_id='1.124691542', rc=None, bet_delay=0, betting_type='ODDS', bsp_market=False, bsp_reconciled=False, complete=True, country_code='GB', cross_matching=False, discount_allowed=True, event_id='27786741', event_name='Ling (RFC) 6th May', event_type_id='7', in_play=False, market_base_rate=5.0, market_time='2016-05-06T12:30:00.000Z', market_type='REV_FORECAST', number_of_active_runners=36, number_of_winners=1, open_date='2016-05-06T12:30:00.000Z', persistence_enabled=True, runners_voidable=False, settled_time=None, status='OPEN', suspend_time='2016-05-06T12:30:00.000Z', timezone='Europe/London', turn_in_play_enabled=False, version=1308185913, market_name='Reverse FC', regulators=['MR_INT'], runners=[Row(adjustmentFactor=None, bsp=None, id=2521889, name='1 - 2 / 2 - 1', removalDate=None, sortPriority=1, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521890, name='1 - 3 / 3 - 1', removalDate=None, sortPriority=2, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521892, name='1 - 4 / 4 - 1', removalDate=None, sortPriority=3, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521899, name='1 - 6 / 6 - 1', removalDate=None, sortPriority=4, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521904, name='1 - 7 / 7 - 1', removalDate=None, sortPriority=5, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521913, name='1 - 8 / 8 - 1', removalDate=None, sortPriority=6, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522761, name='1 - 9 / 9 - 1', removalDate=None, sortPriority=7, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648359, name='1- 10 / 10 - 1', removalDate=None, sortPriority=8, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521891, name='2 - 3 / 3 - 2', removalDate=None, sortPriority=9, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521893, name='2 - 4 / 4 - 2', removalDate=None, sortPriority=10, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521900, name='2 - 6 / 6 - 2', removalDate=None, sortPriority=11, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521905, name='2 - 7 / 7 - 2', removalDate=None, sortPriority=12, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521914, name='2 - 8 / 8 - 2', removalDate=None, sortPriority=13, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522762, name='2 - 9 / 9 - 2', removalDate=None, sortPriority=14, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648362, name='2 - 10 / 10 - 2', removalDate=None, sortPriority=15, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521894, name='3 - 4 / 4 - 3', removalDate=None, sortPriority=16, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521901, name='3 - 6 / 6 - 3', removalDate=None, sortPriority=17, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521908, name='3 - 7 / 7 - 3', removalDate=None, sortPriority=18, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521915, name='3 - 8 / 8 - 3', removalDate=None, sortPriority=19, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522763, name='3 - 9 / 9 - 3', removalDate=None, sortPriority=20, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648363, name='3 - 10 / 10 - 3', removalDate=None, sortPriority=21, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521902, name='4 - 6 / 6 - 4', removalDate=None, sortPriority=22, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521909, name='4 - 7 / 7 - 4', removalDate=None, sortPriority=23, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521916, name='4 - 8 / 8 - 4', removalDate=None, sortPriority=24, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522764, name='4 - 9 / 9 - 4', removalDate=None, sortPriority=25, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648364, name='4 - 10 / 10 - 4', removalDate=None, sortPriority=26, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521911, name='6 - 7 / 7 - 6', removalDate=None, sortPriority=27, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521918, name='6 - 8 / 8 - 6', removalDate=None, sortPriority=28, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522766, name='6 - 9 / 9 - 6', removalDate=None, sortPriority=29, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648368, name='6 - 10 / 10 - 6', removalDate=None, sortPriority=30, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521919, name='7 - 8 / 8 - 7', removalDate=None, sortPriority=31, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522768, name='7 - 9 / 9 - 7', removalDate=None, sortPriority=32, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648371, name='7 - 10 / 10 - 7', removalDate=None, sortPriority=33, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522769, name='8 - 9 / 9 - 8', removalDate=None, sortPriority=34, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648372, name='8 - 10 / 10 - 8', removalDate=None, sortPriority=35, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648374, name='9 - 10 / 10 - 9', removalDate=None, sortPriority=36, status='ACTIVE')])]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runners_only.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the Runners array.\n",
    "runners_exploded = runners_only.select(market_definitions.operation_type,\n",
    "                                       market_definitions.published_time,\n",
    "                                       market_definitions.market_id,\n",
    "                                       market_definitions.market_type,\n",
    "                                       market_definitions.market_name,\n",
    "                                       market_definitions.event_id,\n",
    "                                       market_definitions.event_name,\n",
    "                                       explode(market_definitions.runners).alias('runner_row'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(operation_type='mcm', published_time=1462526770248, market_id='1.124691542', market_type='REV_FORECAST', market_name='Reverse FC', event_id='27786741', event_name='Ling (RFC) 6th May', runner_row=Row(adjustmentFactor=None, bsp=None, id=2521889, name='1 - 2 / 2 - 1', removalDate=None, sortPriority=1, status='ACTIVE'))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runners_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the useful fields, and give them user friendly names.\n",
    "runners = runners_exploded.selectExpr('operation_type',\n",
    "                                      'published_time',\n",
    "                                      'market_id',\n",
    "                                      'market_type',\n",
    "                                      'market_name',\n",
    "                                      'event_id',\n",
    "                                      'event_name',\n",
    "                                      'runner_row.id AS runner_id',\n",
    "                                      'runner_row.name AS runner_name',\n",
    "                                      'runner_row.status AS runner_status',\n",
    "                                      'runner_row.sortPriority AS runner_sort_priority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(operation_type='mcm', published_time=1462526770248, market_id='1.124691542', market_type='REV_FORECAST', market_name='Reverse FC', event_id='27786741', event_name='Ling (RFC) 6th May', runner_id=2521889, runner_name='1 - 2 / 2 - 1', runner_status='ACTIVE', runner_sort_priority=1)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runners.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to only the records that have rc (runner changes).\n",
    "rc_only = mc_exploded.filter(mc_exploded.mc_row.rc.isNotNull()) and mc_exploded.where(mc_exploded.mc_row.id == \"1.124691542\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_id=Row(oid='619e9dd8677066b2230c1e02'), clk='2461683065', mc=[Row(id='1.124691542', marketDefinition=Row(betDelay=0, bettingType='ODDS', bspMarket=False, bspReconciled=False, complete=True, countryCode='GB', crossMatching=False, discountAllowed=True, eachWayDivisor=None, eventId='27786741', eventName='Ling (RFC) 6th May', eventTypeId='7', inPlay=False, marketBaseRate=5.0, marketTime='2016-05-06T12:30:00.000Z', marketType='REV_FORECAST', name='Reverse FC', numberOfActiveRunners=36, numberOfWinners=1, openDate='2016-05-06T12:30:00.000Z', persistenceEnabled=True, regulators=['MR_INT'], runners=[Row(adjustmentFactor=None, bsp=None, id=2521889, name='1 - 2 / 2 - 1', removalDate=None, sortPriority=1, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521890, name='1 - 3 / 3 - 1', removalDate=None, sortPriority=2, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521892, name='1 - 4 / 4 - 1', removalDate=None, sortPriority=3, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521899, name='1 - 6 / 6 - 1', removalDate=None, sortPriority=4, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521904, name='1 - 7 / 7 - 1', removalDate=None, sortPriority=5, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521913, name='1 - 8 / 8 - 1', removalDate=None, sortPriority=6, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522761, name='1 - 9 / 9 - 1', removalDate=None, sortPriority=7, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648359, name='1- 10 / 10 - 1', removalDate=None, sortPriority=8, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521891, name='2 - 3 / 3 - 2', removalDate=None, sortPriority=9, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521893, name='2 - 4 / 4 - 2', removalDate=None, sortPriority=10, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521900, name='2 - 6 / 6 - 2', removalDate=None, sortPriority=11, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521905, name='2 - 7 / 7 - 2', removalDate=None, sortPriority=12, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521914, name='2 - 8 / 8 - 2', removalDate=None, sortPriority=13, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522762, name='2 - 9 / 9 - 2', removalDate=None, sortPriority=14, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648362, name='2 - 10 / 10 - 2', removalDate=None, sortPriority=15, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521894, name='3 - 4 / 4 - 3', removalDate=None, sortPriority=16, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521901, name='3 - 6 / 6 - 3', removalDate=None, sortPriority=17, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521908, name='3 - 7 / 7 - 3', removalDate=None, sortPriority=18, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521915, name='3 - 8 / 8 - 3', removalDate=None, sortPriority=19, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522763, name='3 - 9 / 9 - 3', removalDate=None, sortPriority=20, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648363, name='3 - 10 / 10 - 3', removalDate=None, sortPriority=21, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521902, name='4 - 6 / 6 - 4', removalDate=None, sortPriority=22, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521909, name='4 - 7 / 7 - 4', removalDate=None, sortPriority=23, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521916, name='4 - 8 / 8 - 4', removalDate=None, sortPriority=24, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522764, name='4 - 9 / 9 - 4', removalDate=None, sortPriority=25, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648364, name='4 - 10 / 10 - 4', removalDate=None, sortPriority=26, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521911, name='6 - 7 / 7 - 6', removalDate=None, sortPriority=27, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521918, name='6 - 8 / 8 - 6', removalDate=None, sortPriority=28, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522766, name='6 - 9 / 9 - 6', removalDate=None, sortPriority=29, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648368, name='6 - 10 / 10 - 6', removalDate=None, sortPriority=30, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521919, name='7 - 8 / 8 - 7', removalDate=None, sortPriority=31, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522768, name='7 - 9 / 9 - 7', removalDate=None, sortPriority=32, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648371, name='7 - 10 / 10 - 7', removalDate=None, sortPriority=33, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522769, name='8 - 9 / 9 - 8', removalDate=None, sortPriority=34, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648372, name='8 - 10 / 10 - 8', removalDate=None, sortPriority=35, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648374, name='9 - 10 / 10 - 9', removalDate=None, sortPriority=36, status='ACTIVE')], runnersVoidable=False, settledTime=None, status='OPEN', suspendTime='2016-05-06T12:30:00.000Z', timezone='Europe/London', turnInPlayEnabled=False, venue='Lingfield', version=1308185913), rc=None), Row(id='1.124691543', marketDefinition=Row(betDelay=0, bettingType='ODDS', bspMarket=False, bspReconciled=False, complete=True, countryCode='GB', crossMatching=False, discountAllowed=True, eachWayDivisor=None, eventId='27786741', eventName='Ling (RFC) 6th May', eventTypeId='7', inPlay=False, marketBaseRate=5.0, marketTime='2016-05-06T13:30:00.000Z', marketType='REV_FORECAST', name='Reverse FC', numberOfActiveRunners=36, numberOfWinners=1, openDate='2016-05-06T12:30:00.000Z', persistenceEnabled=True, regulators=['MR_INT'], runners=[Row(adjustmentFactor=None, bsp=None, id=2521889, name='1 - 2 / 2 - 1', removalDate=None, sortPriority=1, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521890, name='1 - 3 / 3 - 1', removalDate=None, sortPriority=2, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521892, name='1 - 4 / 4 - 1', removalDate=None, sortPriority=3, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521895, name='1 - 5 / 5 - 1', removalDate=None, sortPriority=4, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521899, name='1 - 6 / 6 - 1', removalDate=None, sortPriority=5, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521904, name='1 - 7 / 7 - 1', removalDate=None, sortPriority=6, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521913, name='1 - 8 / 8 - 1', removalDate=None, sortPriority=7, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522761, name='1 - 9 / 9 - 1', removalDate=None, sortPriority=8, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521891, name='2 - 3 / 3 - 2', removalDate=None, sortPriority=9, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521893, name='2 - 4 / 4 - 2', removalDate=None, sortPriority=10, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521896, name='2 - 5 / 5 - 2', removalDate=None, sortPriority=11, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521900, name='2 - 6 / 6 - 2', removalDate=None, sortPriority=12, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521905, name='2 - 7 / 7 - 2', removalDate=None, sortPriority=13, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521914, name='2 - 8 / 8 - 2', removalDate=None, sortPriority=14, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522762, name='2 - 9 / 9 - 2', removalDate=None, sortPriority=15, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521894, name='3 - 4 / 4 - 3', removalDate=None, sortPriority=16, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521897, name='3 - 5 / 5 - 3', removalDate=None, sortPriority=17, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521901, name='3 - 6 / 6 - 3', removalDate=None, sortPriority=18, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521908, name='3 - 7 / 7 - 3', removalDate=None, sortPriority=19, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521915, name='3 - 8 / 8 - 3', removalDate=None, sortPriority=20, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522763, name='3 - 9 / 9 - 3', removalDate=None, sortPriority=21, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521898, name='4 - 5 / 5 - 4', removalDate=None, sortPriority=22, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521902, name='4 - 6 / 6 - 4', removalDate=None, sortPriority=23, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521909, name='4 - 7 / 7 - 4', removalDate=None, sortPriority=24, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521916, name='4 - 8 / 8 - 4', removalDate=None, sortPriority=25, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522764, name='4 - 9 / 9 - 4', removalDate=None, sortPriority=26, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521903, name='5 - 6 / 6 - 5', removalDate=None, sortPriority=27, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521910, name='5 - 7 / 7 - 5', removalDate=None, sortPriority=28, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521917, name='5 - 8 / 8 - 5', removalDate=None, sortPriority=29, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522765, name='5 - 9 / 9 - 5', removalDate=None, sortPriority=30, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521911, name='6 - 7 / 7 - 6', removalDate=None, sortPriority=31, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521918, name='6 - 8 / 8 - 6', removalDate=None, sortPriority=32, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522766, name='6 - 9 / 9 - 6', removalDate=None, sortPriority=33, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521919, name='7 - 8 / 8 - 7', removalDate=None, sortPriority=34, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522768, name='7 - 9 / 9 - 7', removalDate=None, sortPriority=35, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522769, name='8 - 9 / 9 - 8', removalDate=None, sortPriority=36, status='ACTIVE')], runnersVoidable=False, settledTime=None, status='OPEN', suspendTime='2016-05-06T13:30:00.000Z', timezone='Europe/London', turnInPlayEnabled=False, venue='Lingfield', version=1308185915), rc=None), Row(id='1.124691544', marketDefinition=Row(betDelay=0, bettingType='ODDS', bspMarket=False, bspReconciled=False, complete=True, countryCode='GB', crossMatching=False, discountAllowed=True, eachWayDivisor=None, eventId='27786741', eventName='Ling (RFC) 6th May', eventTypeId='7', inPlay=False, marketBaseRate=5.0, marketTime='2016-05-06T15:40:00.000Z', marketType='REV_FORECAST', name='Reverse FC', numberOfActiveRunners=28, numberOfWinners=1, openDate='2016-05-06T12:30:00.000Z', persistenceEnabled=True, regulators=['MR_INT'], runners=[Row(adjustmentFactor=None, bsp=None, id=2521889, name='1 - 2 / 2 - 1', removalDate=None, sortPriority=1, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521890, name='1 - 3 / 3 - 1', removalDate=None, sortPriority=2, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521892, name='1 - 4 / 4 - 1', removalDate=None, sortPriority=3, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521895, name='1 - 5 / 5 - 1', removalDate=None, sortPriority=4, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521899, name='1 - 6 / 6 - 1', removalDate=None, sortPriority=5, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521904, name='1 - 7 / 7 - 1', removalDate=None, sortPriority=6, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521913, name='1 - 8 / 8 - 1', removalDate=None, sortPriority=7, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521891, name='2 - 3 / 3 - 2', removalDate=None, sortPriority=8, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521893, name='2 - 4 / 4 - 2', removalDate=None, sortPriority=9, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521896, name='2 - 5 / 5 - 2', removalDate=None, sortPriority=10, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521900, name='2 - 6 / 6 - 2', removalDate=None, sortPriority=11, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521905, name='2 - 7 / 7 - 2', removalDate=None, sortPriority=12, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521914, name='2 - 8 / 8 - 2', removalDate=None, sortPriority=13, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521894, name='3 - 4 / 4 - 3', removalDate=None, sortPriority=14, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521897, name='3 - 5 / 5 - 3', removalDate=None, sortPriority=15, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521901, name='3 - 6 / 6 - 3', removalDate=None, sortPriority=16, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521908, name='3 - 7 / 7 - 3', removalDate=None, sortPriority=17, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521915, name='3 - 8 / 8 - 3', removalDate=None, sortPriority=18, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521898, name='4 - 5 / 5 - 4', removalDate=None, sortPriority=19, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521902, name='4 - 6 / 6 - 4', removalDate=None, sortPriority=20, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521909, name='4 - 7 / 7 - 4', removalDate=None, sortPriority=21, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521916, name='4 - 8 / 8 - 4', removalDate=None, sortPriority=22, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521903, name='5 - 6 / 6 - 5', removalDate=None, sortPriority=23, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521910, name='5 - 7 / 7 - 5', removalDate=None, sortPriority=24, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521917, name='5 - 8 / 8 - 5', removalDate=None, sortPriority=25, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521911, name='6 - 7 / 7 - 6', removalDate=None, sortPriority=26, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521918, name='6 - 8 / 8 - 6', removalDate=None, sortPriority=27, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521919, name='7 - 8 / 8 - 7', removalDate=None, sortPriority=28, status='ACTIVE')], runnersVoidable=False, settledTime=None, status='OPEN', suspendTime='2016-05-06T15:40:00.000Z', timezone='Europe/London', turnInPlayEnabled=False, venue='Lingfield', version=1308185917), rc=None)], op='mcm', pt=1462526770248, mc_row=Row(id='1.124691542', marketDefinition=Row(betDelay=0, bettingType='ODDS', bspMarket=False, bspReconciled=False, complete=True, countryCode='GB', crossMatching=False, discountAllowed=True, eachWayDivisor=None, eventId='27786741', eventName='Ling (RFC) 6th May', eventTypeId='7', inPlay=False, marketBaseRate=5.0, marketTime='2016-05-06T12:30:00.000Z', marketType='REV_FORECAST', name='Reverse FC', numberOfActiveRunners=36, numberOfWinners=1, openDate='2016-05-06T12:30:00.000Z', persistenceEnabled=True, regulators=['MR_INT'], runners=[Row(adjustmentFactor=None, bsp=None, id=2521889, name='1 - 2 / 2 - 1', removalDate=None, sortPriority=1, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521890, name='1 - 3 / 3 - 1', removalDate=None, sortPriority=2, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521892, name='1 - 4 / 4 - 1', removalDate=None, sortPriority=3, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521899, name='1 - 6 / 6 - 1', removalDate=None, sortPriority=4, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521904, name='1 - 7 / 7 - 1', removalDate=None, sortPriority=5, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521913, name='1 - 8 / 8 - 1', removalDate=None, sortPriority=6, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522761, name='1 - 9 / 9 - 1', removalDate=None, sortPriority=7, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648359, name='1- 10 / 10 - 1', removalDate=None, sortPriority=8, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521891, name='2 - 3 / 3 - 2', removalDate=None, sortPriority=9, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521893, name='2 - 4 / 4 - 2', removalDate=None, sortPriority=10, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521900, name='2 - 6 / 6 - 2', removalDate=None, sortPriority=11, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521905, name='2 - 7 / 7 - 2', removalDate=None, sortPriority=12, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521914, name='2 - 8 / 8 - 2', removalDate=None, sortPriority=13, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522762, name='2 - 9 / 9 - 2', removalDate=None, sortPriority=14, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648362, name='2 - 10 / 10 - 2', removalDate=None, sortPriority=15, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521894, name='3 - 4 / 4 - 3', removalDate=None, sortPriority=16, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521901, name='3 - 6 / 6 - 3', removalDate=None, sortPriority=17, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521908, name='3 - 7 / 7 - 3', removalDate=None, sortPriority=18, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521915, name='3 - 8 / 8 - 3', removalDate=None, sortPriority=19, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522763, name='3 - 9 / 9 - 3', removalDate=None, sortPriority=20, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648363, name='3 - 10 / 10 - 3', removalDate=None, sortPriority=21, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521902, name='4 - 6 / 6 - 4', removalDate=None, sortPriority=22, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521909, name='4 - 7 / 7 - 4', removalDate=None, sortPriority=23, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521916, name='4 - 8 / 8 - 4', removalDate=None, sortPriority=24, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522764, name='4 - 9 / 9 - 4', removalDate=None, sortPriority=25, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648364, name='4 - 10 / 10 - 4', removalDate=None, sortPriority=26, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521911, name='6 - 7 / 7 - 6', removalDate=None, sortPriority=27, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521918, name='6 - 8 / 8 - 6', removalDate=None, sortPriority=28, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522766, name='6 - 9 / 9 - 6', removalDate=None, sortPriority=29, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648368, name='6 - 10 / 10 - 6', removalDate=None, sortPriority=30, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2521919, name='7 - 8 / 8 - 7', removalDate=None, sortPriority=31, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522768, name='7 - 9 / 9 - 7', removalDate=None, sortPriority=32, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648371, name='7 - 10 / 10 - 7', removalDate=None, sortPriority=33, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2522769, name='8 - 9 / 9 - 8', removalDate=None, sortPriority=34, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648372, name='8 - 10 / 10 - 8', removalDate=None, sortPriority=35, status='ACTIVE'), Row(adjustmentFactor=None, bsp=None, id=2648374, name='9 - 10 / 10 - 9', removalDate=None, sortPriority=36, status='ACTIVE')], runnersVoidable=False, settledTime=None, status='OPEN', suspendTime='2016-05-06T12:30:00.000Z', timezone='Europe/London', turnInPlayEnabled=False, venue='Lingfield', version=1308185913), rc=None))]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc_only.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the rc (runner changes) array.\n",
    "rc_exploded = rc_only.select(rc_only.op.alias('operation_type'),\n",
    "                             rc_only.pt.alias('published_time'),\n",
    "                             rc_only.mc_row.id.alias('market_id'),\n",
    "                             explode(rc_only.mc_row.rc).alias('runner_change_row'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o423.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 4 times, most recent failure: Lost task 0.3 in stage 30.0 (TID 289, 172.20.0.7, executor 0): org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: _gen_alias_828#828\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:74)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:399)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:397)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:350)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:314)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:298)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:74)\n\tat org.apache.spark.sql.execution.GenerateExec.boundGenerator$lzycompute(GenerateExec.scala:75)\n\tat org.apache.spark.sql.execution.GenerateExec.boundGenerator(GenerateExec.scala:75)\n\tat org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$3(GenerateExec.scala:95)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: Couldn't find _gen_alias_828#828 in [op#453,pt#454L,_gen_alias_827#827]\n\tat scala.sys.package$.error(package.scala:30)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.$anonfun$applyOrElse$1(BoundAttribute.scala:81)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 37 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3448)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3445)\n\tat jdk.internal.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: null\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:74)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:399)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:397)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:350)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:314)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:298)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:74)\n\tat org.apache.spark.sql.execution.GenerateExec.boundGenerator$lzycompute(GenerateExec.scala:75)\n\tat org.apache.spark.sql.execution.GenerateExec.boundGenerator(GenerateExec.scala:75)\n\tat org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$3(GenerateExec.scala:95)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: Couldn't find _gen_alias_828#828 in [op#453,pt#454L,_gen_alias_827#827]\n\tat scala.sys.package$.error(package.scala:30)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.$anonfun$applyOrElse$1(BoundAttribute.scala:81)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 37 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-cb969ff8d8b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrc_exploded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1351\u001b[0m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \"\"\"\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \"\"\"\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o423.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 4 times, most recent failure: Lost task 0.3 in stage 30.0 (TID 289, 172.20.0.7, executor 0): org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: _gen_alias_828#828\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:74)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:399)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:397)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:350)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:314)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:298)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:74)\n\tat org.apache.spark.sql.execution.GenerateExec.boundGenerator$lzycompute(GenerateExec.scala:75)\n\tat org.apache.spark.sql.execution.GenerateExec.boundGenerator(GenerateExec.scala:75)\n\tat org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$3(GenerateExec.scala:95)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: Couldn't find _gen_alias_828#828 in [op#453,pt#454L,_gen_alias_827#827]\n\tat scala.sys.package$.error(package.scala:30)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.$anonfun$applyOrElse$1(BoundAttribute.scala:81)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 37 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3448)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3445)\n\tat jdk.internal.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: null\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:74)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:399)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:397)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:350)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:314)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:298)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:74)\n\tat org.apache.spark.sql.execution.GenerateExec.boundGenerator$lzycompute(GenerateExec.scala:75)\n\tat org.apache.spark.sql.execution.GenerateExec.boundGenerator(GenerateExec.scala:75)\n\tat org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$3(GenerateExec.scala:95)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: Couldn't find _gen_alias_828#828 in [op#453,pt#454L,_gen_alias_827#827]\n\tat scala.sys.package$.error(package.scala:30)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.$anonfun$applyOrElse$1(BoundAttribute.scala:81)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 37 more\n"
     ]
    }
   ],
   "source": [
    "rc_exploded.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the useful fields, and give them user friendly names.\n",
    "runner_changes = rc_exploded.selectExpr('operation_type',\n",
    "                                        'published_time',\n",
    "                                        'market_id',\n",
    "                                        'runner_change_row.id AS runner_id',\n",
    "                                        'runner_change_row.ltp AS last_traded_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o436.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 4 times, most recent failure: Lost task 0.3 in stage 31.0 (TID 293, 172.20.0.8, executor 1): org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: _gen_alias_843#843\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:74)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:399)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:397)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:350)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:314)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:298)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:74)\n\tat org.apache.spark.sql.execution.GenerateExec.boundGenerator$lzycompute(GenerateExec.scala:75)\n\tat org.apache.spark.sql.execution.GenerateExec.boundGenerator(GenerateExec.scala:75)\n\tat org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$3(GenerateExec.scala:95)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: Couldn't find _gen_alias_843#843 in [op#453,pt#454L,_gen_alias_842#842]\n\tat scala.sys.package$.error(package.scala:30)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.$anonfun$applyOrElse$1(BoundAttribute.scala:81)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 37 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3448)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3445)\n\tat jdk.internal.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: null\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:74)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:399)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:397)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:350)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:314)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:298)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:74)\n\tat org.apache.spark.sql.execution.GenerateExec.boundGenerator$lzycompute(GenerateExec.scala:75)\n\tat org.apache.spark.sql.execution.GenerateExec.boundGenerator(GenerateExec.scala:75)\n\tat org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$3(GenerateExec.scala:95)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: Couldn't find _gen_alias_843#843 in [op#453,pt#454L,_gen_alias_842#842]\n\tat scala.sys.package$.error(package.scala:30)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.$anonfun$applyOrElse$1(BoundAttribute.scala:81)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 37 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-3285bd6f01ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrunner_changes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \"\"\"\n\u001b[1;32m   1350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1351\u001b[0m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \"\"\"\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \"\"\"\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o436.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 4 times, most recent failure: Lost task 0.3 in stage 31.0 (TID 293, 172.20.0.8, executor 1): org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: _gen_alias_843#843\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:74)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:399)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:397)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:350)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:314)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:298)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:74)\n\tat org.apache.spark.sql.execution.GenerateExec.boundGenerator$lzycompute(GenerateExec.scala:75)\n\tat org.apache.spark.sql.execution.GenerateExec.boundGenerator(GenerateExec.scala:75)\n\tat org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$3(GenerateExec.scala:95)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: Couldn't find _gen_alias_843#843 in [op#453,pt#454L,_gen_alias_842#842]\n\tat scala.sys.package$.error(package.scala:30)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.$anonfun$applyOrElse$1(BoundAttribute.scala:81)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 37 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3448)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3445)\n\tat jdk.internal.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: null\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:74)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:399)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:397)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:350)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:314)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:298)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:74)\n\tat org.apache.spark.sql.execution.GenerateExec.boundGenerator$lzycompute(GenerateExec.scala:75)\n\tat org.apache.spark.sql.execution.GenerateExec.boundGenerator(GenerateExec.scala:75)\n\tat org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$3(GenerateExec.scala:95)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: Couldn't find _gen_alias_843#843 in [op#453,pt#454L,_gen_alias_842#842]\n\tat scala.sys.package$.error(package.scala:30)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.$anonfun$applyOrElse$1(BoundAttribute.scala:81)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 37 more\n"
     ]
    }
   ],
   "source": [
    "runner_changes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one market_id \n",
    "selection_condition = \"market_id = '1.124691542'\" # \"market_id = '1.124664295'\"\n",
    "\n",
    "chosen_runners = runners.filter(selection_condition).selectExpr('market_id', \n",
    "                                                                'event_name', \n",
    "                                                                'market_name', \n",
    "                                                                'runner_id', \n",
    "                                                                'runner_name').distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-6751ee418150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Runners have a 1:many relationship to Runner Changes, so do an inner join from Runners to Runner Changes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m chosen_runner_changes = chosen_runners.join(runner_changes, chosen_runners.runner_id == runner_changes.runner_id).select(\n\u001b[0m\u001b[1;32m      3\u001b[0m                         \u001b[0mchosen_runners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarket_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mchosen_runners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0mchosen_runners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarket_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \"\"\"\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Runners have a 1:many relationship to Runner Changes, so do an inner join from Runners to Runner Changes.\n",
    "chosen_runner_changes = chosen_runners.join(runner_changes, chosen_runners.runner_id == runner_changes.runner_id).select(\n",
    "                        chosen_runners.market_id, \n",
    "                        chosen_runners.event_name, \n",
    "                        chosen_runners.market_name, \n",
    "                        chosen_runners.runner_id, \n",
    "                        chosen_runners.runner_name,\n",
    "                        runner_changes.published_time,\n",
    "                        runner_changes.last_traded_price\n",
    "                        ).orderBy('published_time').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betfair time seems to be in milliseconds, so this converts it to normal Unix epoch time in seconds.\n",
    "def betfair_to_epoch(betfair_time):\n",
    "    return betfair_time / 1000\n",
    "\n",
    "# Convert a string like '30.08.2018 17:30:00' to a time number.\n",
    "def string_to_epoch(time_str):\n",
    "    pattern = '%d.%m.%Y %H:%M:%S'\n",
    "    return time.mktime(time.strptime(time_str, pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### stephen's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.select(\"_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+---+-------------+\n",
      "|                 _id|       clk|                  mc| op|           pt|\n",
      "+--------------------+----------+--------------------+---+-------------+\n",
      "|[619e9dd8677066b2...|2480088678|[[1.124699287, [0...|mcm|1462537993283|\n",
      "|[619e9dd8677066b2...|2480249142|[[1.124699287, [0...|mcm|1462540135101|\n",
      "|[619e9dd8677066b2...|2481803835|[[1.124699287, [0...|mcm|1462555384075|\n",
      "|[619e9dd8677066b2...|2481994285|[[1.124699287,, [...|mcm|1462557371191|\n",
      "|[619e9dd8677066b2...|2482055516|[[1.124699287,, [...|mcm|1462557910955|\n",
      "|[619e9dd8677066b2...|2482072939|[[1.124699287,, [...|mcm|1462558089056|\n",
      "|[619e9dd8677066b2...|2482078964|[[1.124699287,, [...|mcm|1462558150717|\n",
      "|[619e9dd8677066b2...|2482083965|[[1.124699287, [0...|mcm|1462558199155|\n",
      "|[619e9dd8677066b2...|2482177634|[[1.124699287, [0...|mcm|1462559078531|\n",
      "|[619e9dd8677066b2...|2480088678|[[1.124699285, [0...|mcm|1462537993283|\n",
      "+--------------------+----------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o54.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 4 times, most recent failure: Lost task 1.3 in stage 3.0 (TID 132, 172.20.0.8, executor 1): com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast STRING into a DoubleType (value: BsonString{value='NaN'})\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:214)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$convertToDataType$2(MapFunctions.scala:198)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$convertToDataType$2(MapFunctions.scala:198)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MongoRelation.$anonfun$buildScan$5(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:2979)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:2978)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2978)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast STRING into a DoubleType (value: BsonString{value='NaN'})\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:214)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$convertToDataType$2(MapFunctions.scala:198)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$convertToDataType$2(MapFunctions.scala:198)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MongoRelation.$anonfun$buildScan$5(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-881b90334d39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \"\"\"\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o54.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 4 times, most recent failure: Lost task 1.3 in stage 3.0 (TID 132, 172.20.0.8, executor 1): com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast STRING into a DoubleType (value: BsonString{value='NaN'})\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:214)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$convertToDataType$2(MapFunctions.scala:198)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$convertToDataType$2(MapFunctions.scala:198)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MongoRelation.$anonfun$buildScan$5(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:2979)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:2978)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2978)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast STRING into a DoubleType (value: BsonString{value='NaN'})\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:214)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$convertToDataType$2(MapFunctions.scala:198)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$convertToDataType$2(MapFunctions.scala:198)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MongoRelation.$anonfun$buildScan$5(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
