{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook2\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"5g\").\\\n",
    "        config(\"spark.mongodb.input.uri\",\"mongodb://mongo1:27017,mongo2:27018,mongo3:27019/database.horses_collection?replicaSet=rs0\").\\\n",
    "        config(\"spark.mongodb.output.uri\",\"mongodb://mongo1:27017,mongo2:27018,mongo3:27019/database.horses_collection?replicaSet=rs0\").\\\n",
    "        config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\").\\\n",
    "        getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#reading dataframes from MongoDB\n",
    "df = spark.read.format(\"mongo\").load()\n",
    "df.createOrReplaceTempView(\"mongo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- clk: string (nullable = true)\n",
      " |-- mc: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- marketDefinition: struct (nullable = true)\n",
      " |    |    |    |-- betDelay: integer (nullable = true)\n",
      " |    |    |    |-- bettingType: string (nullable = true)\n",
      " |    |    |    |-- bspMarket: boolean (nullable = true)\n",
      " |    |    |    |-- bspReconciled: boolean (nullable = true)\n",
      " |    |    |    |-- complete: boolean (nullable = true)\n",
      " |    |    |    |-- countryCode: string (nullable = true)\n",
      " |    |    |    |-- crossMatching: boolean (nullable = true)\n",
      " |    |    |    |-- discountAllowed: boolean (nullable = true)\n",
      " |    |    |    |-- eachWayDivisor: double (nullable = true)\n",
      " |    |    |    |-- eventId: string (nullable = true)\n",
      " |    |    |    |-- eventName: string (nullable = true)\n",
      " |    |    |    |-- eventTypeId: string (nullable = true)\n",
      " |    |    |    |-- inPlay: boolean (nullable = true)\n",
      " |    |    |    |-- marketBaseRate: double (nullable = true)\n",
      " |    |    |    |-- marketTime: string (nullable = true)\n",
      " |    |    |    |-- marketType: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- numberOfActiveRunners: integer (nullable = true)\n",
      " |    |    |    |-- numberOfWinners: integer (nullable = true)\n",
      " |    |    |    |-- openDate: string (nullable = true)\n",
      " |    |    |    |-- persistenceEnabled: boolean (nullable = true)\n",
      " |    |    |    |-- regulators: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- runners: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- adjustmentFactor: double (nullable = true)\n",
      " |    |    |    |    |    |-- bsp: double (nullable = true)\n",
      " |    |    |    |    |    |-- id: integer (nullable = true)\n",
      " |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |-- removalDate: string (nullable = true)\n",
      " |    |    |    |    |    |-- sortPriority: integer (nullable = true)\n",
      " |    |    |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- runnersVoidable: boolean (nullable = true)\n",
      " |    |    |    |-- settledTime: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- suspendTime: string (nullable = true)\n",
      " |    |    |    |-- timezone: string (nullable = true)\n",
      " |    |    |    |-- turnInPlayEnabled: boolean (nullable = true)\n",
      " |    |    |    |-- venue: string (nullable = true)\n",
      " |    |    |    |-- version: integer (nullable = true)\n",
      " |    |    |-- rc: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- ltp: double (nullable = true)\n",
      " |    |    |    |    |-- id: integer (nullable = true)\n",
      " |-- op: string (nullable = true)\n",
      " |-- pt: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+---+-------------+\n",
      "|                 _id|       clk|                  mc| op|           pt|\n",
      "+--------------------+----------+--------------------+---+-------------+\n",
      "|[619e9dd8677066b2...|2480088678|[[1.124699287, [0...|mcm|1462537993283|\n",
      "|[619e9dd8677066b2...|2480249142|[[1.124699287, [0...|mcm|1462540135101|\n",
      "|[619e9dd8677066b2...|2481803835|[[1.124699287, [0...|mcm|1462555384075|\n",
      "|[619e9dd8677066b2...|2481994285|[[1.124699287,, [...|mcm|1462557371191|\n",
      "|[619e9dd8677066b2...|2482055516|[[1.124699287,, [...|mcm|1462557910955|\n",
      "|[619e9dd8677066b2...|2482072939|[[1.124699287,, [...|mcm|1462558089056|\n",
      "|[619e9dd8677066b2...|2482078964|[[1.124699287,, [...|mcm|1462558150717|\n",
      "|[619e9dd8677066b2...|2482083965|[[1.124699287, [0...|mcm|1462558199155|\n",
      "|[619e9dd8677066b2...|2482177634|[[1.124699287, [0...|mcm|1462559078531|\n",
      "|[619e9dd8677066b2...|2480088678|[[1.124699285, [0...|mcm|1462537993283|\n",
      "|[619e9dd8677066b2...|2480212422|[[1.124699285, [0...|mcm|1462539800697|\n",
      "|[619e9dd8677066b2...|2480409166|[[1.124699285, [0...|mcm|1462541919372|\n",
      "|[619e9dd8677066b2...|2481526806|[[1.124699285,, [...|mcm|1462552870198|\n",
      "|[619e9dd8677066b2...|2481622628|[[1.124699285, [0...|mcm|1462553643217|\n",
      "|[619e9dd8677066b2...|2481628092|[[1.124699285, [0...|mcm|1462553705002|\n",
      "|[619e9dd8677066b2...|2481633423|[[1.124699285, [0...|mcm|1462553765802|\n",
      "|[619e9dd8677066b2...|2481879973|[[1.124699285, [0...|mcm|1462556181157|\n",
      "|[619e9dd8677066b2...|2481938295|[[1.124699285, [0...|mcm|1462556825096|\n",
      "|[619e9dd8677066b2...|2480088678|[[1.124699278, [0...|mcm|1462537993283|\n",
      "|[619e9dd8677066b2...|2480180647|[[1.124699278, [0...|mcm|1462539270638|\n",
      "+--------------------+----------+--------------------+---+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"select * from mongo array_contains(root.mc, array('CLOSED'))\").show()\n",
    "from pyspark.sql.functions import explode   # Explodes lists into rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_exploded = df.select('*', explode(df.mc).alias('mc_row'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_only = mc_exploded.filter(mc_exploded.mc_row.marketDefinition.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+---+-------------+--------------------+\n",
      "|                 _id|       clk|                  mc| op|           pt|              mc_row|\n",
      "+--------------------+----------+--------------------+---+-------------+--------------------+\n",
      "|[619e9dd8677066b2...|2480088678|[[1.124699287, [0...|mcm|1462537993283|[1.124699287, [0,...|\n",
      "|[619e9dd8677066b2...|2480249142|[[1.124699287, [0...|mcm|1462540135101|[1.124699287, [0,...|\n",
      "|[619e9dd8677066b2...|2481803835|[[1.124699287, [0...|mcm|1462555384075|[1.124699287, [0,...|\n",
      "|[619e9dd8677066b2...|2482083965|[[1.124699287, [0...|mcm|1462558199155|[1.124699287, [0,...|\n",
      "|[619e9dd8677066b2...|2482177634|[[1.124699287, [0...|mcm|1462559078531|[1.124699287, [0,...|\n",
      "|[619e9dd8677066b2...|2480088678|[[1.124699285, [0...|mcm|1462537993283|[1.124699285, [0,...|\n",
      "|[619e9dd8677066b2...|2480212422|[[1.124699285, [0...|mcm|1462539800697|[1.124699285, [0,...|\n",
      "|[619e9dd8677066b2...|2480409166|[[1.124699285, [0...|mcm|1462541919372|[1.124699285, [0,...|\n",
      "|[619e9dd8677066b2...|2481622628|[[1.124699285, [0...|mcm|1462553643217|[1.124699285, [0,...|\n",
      "|[619e9dd8677066b2...|2481628092|[[1.124699285, [0...|mcm|1462553705002|[1.124699285, [0,...|\n",
      "|[619e9dd8677066b2...|2481633423|[[1.124699285, [0...|mcm|1462553765802|[1.124699285, [0,...|\n",
      "|[619e9dd8677066b2...|2481879973|[[1.124699285, [0...|mcm|1462556181157|[1.124699285, [0,...|\n",
      "|[619e9dd8677066b2...|2481938295|[[1.124699285, [0...|mcm|1462556825096|[1.124699285, [0,...|\n",
      "|[619e9dd8677066b2...|2480088678|[[1.124699278, [0...|mcm|1462537993283|[1.124699278, [0,...|\n",
      "|[619e9dd8677066b2...|2480180647|[[1.124699278, [0...|mcm|1462539270638|[1.124699278, [0,...|\n",
      "|[619e9dd8677066b2...|2481407254|[[1.124699278, [0...|mcm|1462551952407|[1.124699278, [0,...|\n",
      "|[619e9dd8677066b2...|2481698341|[[1.124699278, [0...|mcm|1462554327642|[1.124699278, [0,...|\n",
      "|[619e9dd8677066b2...|2481740307|[[1.124699278, [0...|mcm|1462554768806|[1.124699278, [0,...|\n",
      "|[619e9dd8677066b2...|2480088678|[[1.124699287, [0...|mcm|1462537993283|[1.124699287, [0,...|\n",
      "|[619e9dd8677066b2...|2480088678|[[1.124699287, [0...|mcm|1462537993283|[1.124699278, [0,...|\n",
      "+--------------------+----------+--------------------+---+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "md_only.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_id=Row(oid='619e9dd8677066b2230c1dcb'), clk='2480088678', mc=[Row(id='1.124699287', marketDefinition=Row(betDelay=0, bettingType='ODDS', bspMarket=False, bspReconciled=False, complete=True, countryCode='IE', crossMatching=False, discountAllowed=True, eachWayDivisor=None, eventId='27787006', eventName='Cork (W/O) 6th May', eventTypeId='7', inPlay=False, marketBaseRate=5.0, marketTime='2016-05-06T18:10:00.000Z', marketType='WITHOUT_FAV', name='Without Ah Littleluck', numberOfActiveRunners=14, numberOfWinners=1, openDate='2016-05-06T17:05:00.000Z', persistenceEnabled=True, regulators=['MR_INT'], runners=[Row(adjustmentFactor=23.82, bsp=None, id=9926956, name='Icantsay', removalDate=None, sortPriority=1, status='ACTIVE'), Row(adjustmentFactor=21.77, bsp=None, id=10914088, name='Emily Anna', removalDate=None, sortPriority=2, status='ACTIVE'), Row(adjustmentFactor=20.04, bsp=None, id=10889402, name='Donegal Tuesday', removalDate=None, sortPriority=3, status='ACTIVE'), Row(adjustmentFactor=13.45, bsp=None, id=9811690, name='Even Though', removalDate=None, sortPriority=4, status='ACTIVE'), Row(adjustmentFactor=3.81, bsp=None, id=9273655, name='Robbina', removalDate=None, sortPriority=5, status='ACTIVE'), Row(adjustmentFactor=3.13, bsp=None, id=10768750, name='Iconic Image', removalDate=None, sortPriority=6, status='ACTIVE'), Row(adjustmentFactor=3.13, bsp=None, id=10757853, name='Shanaway', removalDate=None, sortPriority=7, status='ACTIVE'), Row(adjustmentFactor=1.86, bsp=None, id=9081103, name='Sunchyme', removalDate=None, sortPriority=8, status='ACTIVE'), Row(adjustmentFactor=1.86, bsp=None, id=11137650, name='Barrymore Legal', removalDate=None, sortPriority=9, status='ACTIVE'), Row(adjustmentFactor=1.86, bsp=None, id=10643462, name='Bean An Tobar', removalDate=None, sortPriority=10, status='ACTIVE'), Row(adjustmentFactor=1.28, bsp=None, id=10283597, name='Westfield King', removalDate=None, sortPriority=11, status='ACTIVE'), Row(adjustmentFactor=1.28, bsp=None, id=9532544, name='Courtin Bb', removalDate=None, sortPriority=12, status='ACTIVE'), Row(adjustmentFactor=1.28, bsp=None, id=10571006, name='A Lattaque', removalDate=None, sortPriority=13, status='ACTIVE'), Row(adjustmentFactor=1.28, bsp=None, id=10757846, name='Mount Russell', removalDate=None, sortPriority=14, status='ACTIVE')], runnersVoidable=False, settledTime=None, status='OPEN', suspendTime='2016-05-06T18:10:00.000Z', timezone='Europe/London', turnInPlayEnabled=False, venue='Cork', version=1308268140), rc=None)], op='mcm', pt=1462537993283, mc_row=Row(id='1.124699287', marketDefinition=Row(betDelay=0, bettingType='ODDS', bspMarket=False, bspReconciled=False, complete=True, countryCode='IE', crossMatching=False, discountAllowed=True, eachWayDivisor=None, eventId='27787006', eventName='Cork (W/O) 6th May', eventTypeId='7', inPlay=False, marketBaseRate=5.0, marketTime='2016-05-06T18:10:00.000Z', marketType='WITHOUT_FAV', name='Without Ah Littleluck', numberOfActiveRunners=14, numberOfWinners=1, openDate='2016-05-06T17:05:00.000Z', persistenceEnabled=True, regulators=['MR_INT'], runners=[Row(adjustmentFactor=23.82, bsp=None, id=9926956, name='Icantsay', removalDate=None, sortPriority=1, status='ACTIVE'), Row(adjustmentFactor=21.77, bsp=None, id=10914088, name='Emily Anna', removalDate=None, sortPriority=2, status='ACTIVE'), Row(adjustmentFactor=20.04, bsp=None, id=10889402, name='Donegal Tuesday', removalDate=None, sortPriority=3, status='ACTIVE'), Row(adjustmentFactor=13.45, bsp=None, id=9811690, name='Even Though', removalDate=None, sortPriority=4, status='ACTIVE'), Row(adjustmentFactor=3.81, bsp=None, id=9273655, name='Robbina', removalDate=None, sortPriority=5, status='ACTIVE'), Row(adjustmentFactor=3.13, bsp=None, id=10768750, name='Iconic Image', removalDate=None, sortPriority=6, status='ACTIVE'), Row(adjustmentFactor=3.13, bsp=None, id=10757853, name='Shanaway', removalDate=None, sortPriority=7, status='ACTIVE'), Row(adjustmentFactor=1.86, bsp=None, id=9081103, name='Sunchyme', removalDate=None, sortPriority=8, status='ACTIVE'), Row(adjustmentFactor=1.86, bsp=None, id=11137650, name='Barrymore Legal', removalDate=None, sortPriority=9, status='ACTIVE'), Row(adjustmentFactor=1.86, bsp=None, id=10643462, name='Bean An Tobar', removalDate=None, sortPriority=10, status='ACTIVE'), Row(adjustmentFactor=1.28, bsp=None, id=10283597, name='Westfield King', removalDate=None, sortPriority=11, status='ACTIVE'), Row(adjustmentFactor=1.28, bsp=None, id=9532544, name='Courtin Bb', removalDate=None, sortPriority=12, status='ACTIVE'), Row(adjustmentFactor=1.28, bsp=None, id=10571006, name='A Lattaque', removalDate=None, sortPriority=13, status='ACTIVE'), Row(adjustmentFactor=1.28, bsp=None, id=10757846, name='Mount Russell', removalDate=None, sortPriority=14, status='ACTIVE')], runnersVoidable=False, settledTime=None, status='OPEN', suspendTime='2016-05-06T18:10:00.000Z', timezone='Europe/London', turnInPlayEnabled=False, venue='Cork', version=1308268140), rc=None))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_exploded.head(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the marketDefinition array.\n",
    "market_definitions = md_only.selectExpr('op AS operation_type',\n",
    "                                        'clk AS sequence_token',\n",
    "                                        'pt AS published_time',\n",
    "                                        'mc_row.id AS market_id',\n",
    "                                        'mc_row.rc AS rc',\n",
    "                                        'mc_row.marketDefinition.betDelay AS bet_delay',\n",
    "                                        'mc_row.marketDefinition.bettingType AS betting_type',\n",
    "                                        'mc_row.marketDefinition.bspMarket AS bsp_market',\n",
    "                                        'mc_row.marketDefinition.bspReconciled AS bsp_reconciled',\n",
    "                                        'mc_row.marketDefinition.complete AS complete',\n",
    "                                        'mc_row.marketDefinition.countryCode AS country_code',\n",
    "                                        'mc_row.marketDefinition.crossMatching AS cross_matching',\n",
    "                                        'mc_row.marketDefinition.discountAllowed AS discount_allowed',\n",
    "                                        'mc_row.marketDefinition.eventId AS event_id',\n",
    "                                        'mc_row.marketDefinition.eventName AS event_name',\n",
    "                                        'mc_row.marketDefinition.eventTypeId AS event_type_id',\n",
    "                                        'mc_row.marketDefinition.inPlay AS in_play',\n",
    "                                        'mc_row.marketDefinition.marketBaseRate AS market_base_rate',\n",
    "                                        'mc_row.marketDefinition.marketTime AS market_time',\n",
    "                                        'mc_row.marketDefinition.marketType AS market_type',\n",
    "                                        'mc_row.marketDefinition.numberOfActiveRunners AS number_of_active_runners',\n",
    "                                        'mc_row.marketDefinition.numberOfWinners AS number_of_winners',\n",
    "                                        'mc_row.marketDefinition.openDate AS open_date',\n",
    "                                        'mc_row.marketDefinition.persistenceEnabled AS persistence_enabled',\n",
    "                                        'mc_row.marketDefinition.runnersVoidable AS runners_voidable',\n",
    "                                        'mc_row.marketDefinition.settledTime AS settled_time',\n",
    "                                        'mc_row.marketDefinition.status AS status',\n",
    "                                        'mc_row.marketDefinition.suspendTime AS suspend_time',\n",
    "                                        'mc_row.marketDefinition.timezone AS timezone',\n",
    "                                        'mc_row.marketDefinition.turnInPlayEnabled AS turn_in_play_enabled',\n",
    "                                        'mc_row.marketDefinition.version AS version',\n",
    "                                        'mc_row.marketDefinition.name AS market_name',\n",
    "                                        'mc_row.marketDefinition.regulators AS regulators',\n",
    "                                        'mc_row.marketDefinition.runners AS runners')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_definitions = market_definitions.filter(market_definitions[\"status\"]==\"CLOSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_definitions = market_definitions.filter(market_definitions[\"market_type\"]==\"WIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_only = market_definitions.filter(market_definitions.runners.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the Runners array.\n",
    "runners_exploded = runners_only.select(market_definitions.operation_type,\n",
    "                                       market_definitions.published_time,\n",
    "                                       market_definitions.market_id,\n",
    "                                       market_definitions.market_name,\n",
    "                                       market_definitions.event_id,\n",
    "                                       market_definitions.event_name,\n",
    "                                       explode(market_definitions.runners).alias('runner_row'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the useful fields, and give them user friendly names.\n",
    "runners = runners_exploded.selectExpr('operation_type',\n",
    "                                      'published_time',\n",
    "                                      'market_id',\n",
    "                                      'market_name',\n",
    "                                      'event_id',\n",
    "                                      'event_name',\n",
    "                                      'runner_row.id AS runner_id',\n",
    "                                      'runner_row.name AS runner_name',\n",
    "                                      'runner_row.status AS runner_status',\n",
    "                                      'runner_row.sortPriority AS runner_sort_priority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to only the records that have rc (runner changes).\n",
    "rc_only = mc_exploded.filter(mc_exploded.mc_row.rc.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the rc (runner changes) array.\n",
    "rc_exploded = rc_only.select(rc_only.op.alias('operation_type'),\n",
    "                             rc_only.pt.alias('published_time'),\n",
    "                             rc_only.mc_row.id.alias('market_id'),\n",
    "                             explode(rc_only.mc_row.rc).alias('runner_change_row'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the useful fields, and give them user friendly names.\n",
    "runner_changes = rc_exploded.selectExpr('operation_type',\n",
    "                                        'published_time',\n",
    "                                        'market_id',\n",
    "                                        'runner_change_row.id AS runner_id',\n",
    "                                        'runner_change_row.ltp AS last_traded_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-----------+---------------+--------+-------------------+---------+------------------+-------------+--------------------+\n",
      "|operation_type|published_time|  market_id|    market_name|event_id|         event_name|runner_id|       runner_name|runner_status|runner_sort_priority|\n",
      "+--------------+--------------+-----------+---------------+--------+-------------------+---------+------------------+-------------+--------------------+\n",
      "|           mcm| 1462501034557|1.124659130|     R8 1m Allw|27784588|EvangD (US) 5th May| 10861746|          B Brazin|      REMOVED|                   1|\n",
      "|           mcm| 1462501034557|1.124659130|     R8 1m Allw|27784588|EvangD (US) 5th May|  4517788|       Great Value|      REMOVED|                   2|\n",
      "|           mcm| 1462501034557|1.124659130|     R8 1m Allw|27784588|EvangD (US) 5th May|   187960|        Kid Creole|      REMOVED|                   3|\n",
      "|           mcm| 1462501034557|1.124659130|     R8 1m Allw|27784588|EvangD (US) 5th May| 10743810|        Slideruler|      REMOVED|                   4|\n",
      "|           mcm| 1462501034557|1.124659130|     R8 1m Allw|27784588|EvangD (US) 5th May| 10575249|       Bitsys Half|        LOSER|                   5|\n",
      "|           mcm| 1462501034557|1.124659130|     R8 1m Allw|27784588|EvangD (US) 5th May| 10288198|         Blue Code|        LOSER|                   6|\n",
      "|           mcm| 1462501034557|1.124659130|     R8 1m Allw|27784588|EvangD (US) 5th May| 10116126|            Zartan|        LOSER|                   7|\n",
      "|           mcm| 1462501034557|1.124659130|     R8 1m Allw|27784588|EvangD (US) 5th May| 10221428|         Old Conch|        LOSER|                   8|\n",
      "|           mcm| 1462501034557|1.124659130|     R8 1m Allw|27784588|EvangD (US) 5th May| 10150151|Rose Needs A Check|        LOSER|                   9|\n",
      "|           mcm| 1462501034557|1.124659130|     R8 1m Allw|27784588|EvangD (US) 5th May| 10237987|           Make Do|        LOSER|                  10|\n",
      "|           mcm| 1462501034557|1.124659130|     R8 1m Allw|27784588|EvangD (US) 5th May| 10674850|      Cajun Kitten|        LOSER|                  11|\n",
      "|           mcm| 1462501034557|1.124659130|     R8 1m Allw|27784588|EvangD (US) 5th May| 10550376|       Dr. Zipcity|       WINNER|                  12|\n",
      "|           mcm| 1462501034557|1.124659130|     R8 1m Allw|27784588|EvangD (US) 5th May| 10849769|     Goforthegreen|        LOSER|                  13|\n",
      "|           mcm| 1462501034557|1.124659130|     R8 1m Allw|27784588|EvangD (US) 5th May| 10762541|       Specialeyes|        LOSER|                  14|\n",
      "|           mcm| 1462502535129|1.124659133|R9 7f Mdn Claim|27784588|EvangD (US) 5th May| 10175250|           Cabildo|        LOSER|                   1|\n",
      "|           mcm| 1462502535129|1.124659133|R9 7f Mdn Claim|27784588|EvangD (US) 5th May| 10592064|      Point Secret|        LOSER|                   2|\n",
      "|           mcm| 1462502535129|1.124659133|R9 7f Mdn Claim|27784588|EvangD (US) 5th May|  9111702|       Lip Service|       WINNER|                   3|\n",
      "|           mcm| 1462502535129|1.124659133|R9 7f Mdn Claim|27784588|EvangD (US) 5th May| 10154653|         Warlander|        LOSER|                   4|\n",
      "|           mcm| 1462502535129|1.124659133|R9 7f Mdn Claim|27784588|EvangD (US) 5th May| 10059993|    Cowboy Classic|        LOSER|                   5|\n",
      "|           mcm| 1462502535129|1.124659133|R9 7f Mdn Claim|27784588|EvangD (US) 5th May| 10681528|    Dry Wood Creek|        LOSER|                   6|\n",
      "+--------------+--------------+-----------+---------------+--------+-------------------+---------+------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "runners.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.researchgate.net/publication/351844751_Sonic_Thunder_vs_Brian_the_Snail_Are_people_affected_by_uninformative_racehorse_names\n",
    "fast_names = [\n",
    "    'a mile a minute', 'helter-skelter', 'quick-fire',\n",
    "    'apace', 'high-speed quickly',\n",
    "    'as fast as your legs would carry you',\n",
    "    'hot', 'quickness',\n",
    "    'as if it is going out of style', 'hotfoot', 'rapid',\n",
    "    'at a rate of knots', 'hustle', 'rapid-fire',\n",
    "    'at full pelt', 'in the twinkling of an eye',\n",
    "    'rate',\n",
    "    'at full speed', 'Jack Robinson', 'say',\n",
    "    'at full tilt', 'lick', 'shot',\n",
    "    'at full tilt', 'lickety-split', 'smartly',\n",
    "    'before you can say Jack Robinson', 'lightning', 'souped-up',\n",
    "    'blistering', 'like a shot', 'spanking',\n",
    "    'breakneck' 'like a streak of lightning', 'speed',\n",
    "    'brisk' 'like lightning spread', 'like wildfire',\n",
    "    'chop-chop', 'meteoric', 'streak',\n",
    "    'crash', 'mile', 'style',\n",
    "    'express', 'nimble', 'superfast',\n",
    "    'fast', 'nimbleness', 'supersonic',\n",
    "\n",
    "    'fleet', 'nimbly', 'swift',\n",
    "    'full', 'nippy', 'swiftly',\n",
    "    'full steam ahead', 'pdq', 'thick',\n",
    "    'gallop', 'pell-mell', 'thick and fast',\n",
    "    'galloping', 'poky', 'tilt',\n",
    "    'go like hot cakes', 'posthaste', 'top',\n",
    "    'have a heavy foot', 'precipitous', 'twinkling',\n",
    "    'headlong', 'precipitously', 'whoosh',\n",
    "    'heavy', 'prompt', 'wildfire',\n",
    "    'hell', 'promptly', 'zippy',\n",
    "    'hell for leather', 'quick',\n",
    "\n",
    "\n",
    "    'accelerated', 'high-speed', 'pell-mell',\n",
    "    'at full speed', 'hurried', 'post-haste',\n",
    "    'at full tilt', 'hurriedly', 'quick',\n",
    "    'at speed', 'in a flash', 'quickly',\n",
    "    'at the speed of light', 'in a hurry', 'rapid',\n",
    "    'blistering', 'in a trice', 'rapidly',\n",
    "    'breakneck', 'in a wink', 'smart',\n",
    "    'brisk', 'in haste', 'speedily',\n",
    "    'briskly', 'in time', 'speedy',\n",
    "    'energetically', 'in no time at all', 'sporty',\n",
    "    'expeditious', 'in the blink of an eye', 'sprightly',\n",
    "    'expeditiously', 'like a flash', 'swift',\n",
    "    'express', 'like a shot', 'swiftly',\n",
    "    'fast', 'like an arrow from a bow', 'turbo',\n",
    "    'fast-moving', 'lively', 'unhesitating',\n",
    "    'fleet-footed', 'meteoric', 'whirlwind',\n",
    "    'flying', 'nimble', 'with all haste',\n",
    "    'hastily', 'on the double', 'with dispatch',\n",
    "    'hasty', 'pell-mell', 'without delay',\n",
    "\n",
    "    'acceleration', 'haste', 'scutter',\n",
    "    'alacrity', 'hasten', 'sharpness',\n",
    "    'blast', 'hurriedness', 'shoot',\n",
    "    'bolt', 'hurry', 'spank along',\n",
    "    'bowl along', 'hurry', 'speed',\n",
    "    'briskness', 'hurtle', 'speediness',\n",
    "    'career', 'immediacy', 'sprint',\n",
    "    'celerity', 'momentum', 'stampede',\n",
    "    'charge', 'pace', 'streak',\n",
    "    'dart', 'precipitateness', 'sweep',\n",
    "    'dash', 'promptness', 'swiftness',\n",
    "    'dispatch', 'quickness', 'swoop',\n",
    "    'expedition', 'race', 'tempo',\n",
    "    'expeditiousness', 'rapidity', 'uzz',\n",
    "    'fastness', 'rate', 'velocity',\n",
    "    'flash', 'rattle along', 'whirl',\n",
    "    'fly', 'run', 'whizz',\n",
    "    'gallop', 'rush', 'whoosh',\n",
    "    'go hell for leather', 'scramble', 'wing',\n",
    "    'go like lightning', 'scud', 'zoom',\n",
    "    'hare', 'scurry',\n",
    "\n",
    "    'abrupt', 'impetuous', 'rushed',\n",
    "    'agility', 'outrun', 'scramble',\n",
    "    'dash', 'overhasty', 'speed',\n",
    "    'disconcerted', 'overrun', 'speedily',\n",
    "    'dodge', 'promptly', 'speedy',\n",
    "    'haste', 'quick', 'sudden',\n",
    "    'hastily', 'quickly', 'suddenly',\n",
    "    'hurried', 'rapid', 'swift',\n",
    "    'hurriedly', 'rapidly', 'swiftly',\n",
    "    'hurry', 'rush', 'zoom',\n",
    "\n",
    "    'accelerate', 'haste', 'race',\n",
    "    'acceleration', 'hasten', 'rapidity',\n",
    "    'agility', 'hie', 'rush',\n",
    "    'airspeed', 'hurriedly', 'speedy',\n",
    "    'celerity', 'hurry', 'stronghold',\n",
    "    'dash', 'pace', 'swift',\n",
    "    'decelerate', 'quick', 'swiftness',\n",
    "    'expedite', 'quicken', 'tempo',\n",
    "    'fast', 'quickly', 'urgently',\n",
    "    'fastness', 'quickness', 'velocity',\n",
    "\n",
    "    'Apache', 'Bentley', 'Blustery',\n",
    "    'Bullet', 'Buzz', 'Comet',\n",
    "    ',Cougar', ',Falcon', 'Faster',\n",
    "    'Flash', 'Ghost', 'rider', 'Harley',\n",
    "    'Jet', 'Jump', 'Jumping',\n",
    "    'Miles', 'Mustang', 'Pony express',\n",
    "    'Quick', 'Quicky', 'Racer',\n",
    "    'Rapid', 'Rapide', 'Rocket',\n",
    "    'Sonic', 'Speedy', 'Taz',\n",
    "    'Tornado', 'Traveler', 'Wildfire',\n",
    "    'Voyager', 'Wild', 'Velocity', \n",
    "\n",
    "    'Sonic Power', 'Speed Dragon', 'Zippy Lad', 'Lightening Vault',\n",
    "    'Powerful Jet', 'Orbit Express', 'Swift Chap', 'Blazing Tempo',\n",
    "    'Brave Falcon', 'Rush Now', 'Top Magic', 'Dixie Flyer',\n",
    "    'Esprit De Bullet', 'Strike Fast', 'Hustle Hard', 'Diamond Rush',\n",
    "    'Crown Me Fast', 'Hot Seat', 'Top Gear', 'Bright Bullet',\n",
    "    'Quick Art', 'Rush Of Blood', 'Top Boy', 'Meteoric',\n",
    "    'Moments',\n",
    "    'One Wild Guy', 'Sonic Thunder', 'Grand Gallop', 'Zippy Speed',\n",
    "    'Run for Roses', 'Saratoga',\n",
    "    'Wildcat',\n",
    "    'Quick Beers', 'Sudden Rush',\n",
    "    'Flyingwithoutwings', 'Fast On', 'Dazzlem Quick', 'You Drive I Fly',\n",
    "    'Irish Rocket', 'Hot Sauce', 'Mighty Flying', 'frost'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_names_lc = (map(lambda s: s.lower(), fast_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@udf(BooleanType())\n",
    "def is_fast(name):\n",
    "    any(name.lower() in s for s in fast_names_lc)\n",
    "    \n",
    "calc_is_fast = udf(is_fast, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners = runners.withColumn('is_fast_horse',  calc_is_fast(runners.runner_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 19:24:53 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 6, 172.22.0.8, executor 1): com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast STRING into a DoubleType (value: BsonString{value='NaN'})\n",
      "\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:214)\n",
      "\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n",
      "\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n",
      "\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n",
      "\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$convertToDataType$2(MapFunctions.scala:198)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:198)\n",
      "\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n",
      "\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n",
      "\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n",
      "\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n",
      "\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n",
      "\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n",
      "\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$convertToDataType$2(MapFunctions.scala:198)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:198)\n",
      "\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n",
      "\tat com.mongodb.spark.sql.MongoRelation.$anonfun$buildScan$5(MongoRelation.scala:58)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1158)\n",
      "\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1174)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1212)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "21/12/07 19:24:59 ERROR TaskSetManager: Task 0 in stage 6.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o157.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0 (TID 9, 172.22.0.7, executor 0): com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast STRING into a DoubleType (value: BsonString{value='NaN'})\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:214)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$convertToDataType$2(MapFunctions.scala:198)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$convertToDataType$2(MapFunctions.scala:198)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MongoRelation.$anonfun$buildScan$5(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1158)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1174)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1212)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3625)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast STRING into a DoubleType (value: BsonString{value='NaN'})\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:214)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$convertToDataType$2(MapFunctions.scala:198)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$convertToDataType$2(MapFunctions.scala:198)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MongoRelation.$anonfun$buildScan$5(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1158)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1174)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1212)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5567/3548035692.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrunners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o157.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0 (TID 9, 172.22.0.7, executor 0): com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast STRING into a DoubleType (value: BsonString{value='NaN'})\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:214)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$convertToDataType$2(MapFunctions.scala:198)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$convertToDataType$2(MapFunctions.scala:198)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MongoRelation.$anonfun$buildScan$5(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1158)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1174)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1212)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3625)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast STRING into a DoubleType (value: BsonString{value='NaN'})\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:214)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$convertToDataType$2(MapFunctions.scala:198)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MapFunctions$.castToStructType(MapFunctions.scala:236)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:208)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$convertToDataType$2(MapFunctions.scala:198)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat com.mongodb.spark.sql.MapFunctions$.convertToDataType(MapFunctions.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$documentToRow$1(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:35)\n\tat com.mongodb.spark.sql.MongoRelation.$anonfun$buildScan$5(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1158)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1174)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1212)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n"
     ]
    }
   ],
   "source": [
    "runners.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast_only = runners.filter(runners.is_fast_horse==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast_only.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/53457975/pyspark-udf-function-error-in-lambda-function\n",
    "import os\n",
    "os.environ['OBJC_DISABLE_INITIALIZE_FORK_SAFETY'] = 'YES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook2\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"5g\").\\\n",
    "        config(\"spark.mongodb.input.uri\",\"mongodb://mongo1:27017,mongo2:27018,mongo3:27019/database.horses_collection?replicaSet=rs0\").\\\n",
    "        config(\"spark.mongodb.output.uri\",\"mongodb://mongo1:27017,mongo2:27018,mongo3:27019/database.horses_collection?replicaSet=rs0\").\\\n",
    "        config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\").\\\n",
    "        getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading dataframes from MongoDB\n",
    "\n",
    "# sampleSize - https://stackoverflow.com/a/56255303\n",
    "df = spark.read.format(\"mongo\").option('sampleSize', 50000).load()\n",
    "df.createOrReplaceTempView(\"mongo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"select * from mongo array_contains(root.mc, array('CLOSED'))\").show()\n",
    "from pyspark.sql.functions import explode   # Explodes lists into rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_exploded = df.select('*', explode(df.mc).alias('mc_row'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_only = mc_exploded.filter(mc_exploded.mc_row.marketDefinition.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_only.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the marketDefinition array.\n",
    "market_definitions = md_only.selectExpr('op AS operation_type',\n",
    "                                        'clk AS sequence_token',\n",
    "                                        'pt AS published_time',\n",
    "                                        'mc_row.id AS market_id',\n",
    "                                        'mc_row.rc AS rc',\n",
    "                                        'mc_row.marketDefinition.betDelay AS bet_delay',\n",
    "                                        'mc_row.marketDefinition.bettingType AS betting_type',\n",
    "                                        'mc_row.marketDefinition.bspMarket AS bsp_market',\n",
    "                                        'mc_row.marketDefinition.bspReconciled AS bsp_reconciled',\n",
    "                                        'mc_row.marketDefinition.complete AS complete',\n",
    "                                        'mc_row.marketDefinition.countryCode AS country_code',\n",
    "                                        'mc_row.marketDefinition.crossMatching AS cross_matching',\n",
    "                                        'mc_row.marketDefinition.discountAllowed AS discount_allowed',\n",
    "                                        'mc_row.marketDefinition.eventId AS event_id',\n",
    "                                        'mc_row.marketDefinition.eventName AS event_name',\n",
    "                                        'mc_row.marketDefinition.eventTypeId AS event_type_id',\n",
    "                                        'mc_row.marketDefinition.inPlay AS in_play',\n",
    "                                        'mc_row.marketDefinition.marketBaseRate AS market_base_rate',\n",
    "                                        'mc_row.marketDefinition.marketTime AS market_time',\n",
    "                                        'mc_row.marketDefinition.marketType AS market_type',\n",
    "                                        'mc_row.marketDefinition.numberOfActiveRunners AS number_of_active_runners',\n",
    "                                        'mc_row.marketDefinition.numberOfWinners AS number_of_winners',\n",
    "                                        'mc_row.marketDefinition.openDate AS open_date',\n",
    "                                        'mc_row.marketDefinition.persistenceEnabled AS persistence_enabled',\n",
    "                                        'mc_row.marketDefinition.runnersVoidable AS runners_voidable',\n",
    "                                        'mc_row.marketDefinition.settledTime AS settled_time',\n",
    "                                        'mc_row.marketDefinition.status AS status',\n",
    "                                        'mc_row.marketDefinition.suspendTime AS suspend_time',\n",
    "                                        'mc_row.marketDefinition.timezone AS timezone',\n",
    "                                        'mc_row.marketDefinition.turnInPlayEnabled AS turn_in_play_enabled',\n",
    "                                        'mc_row.marketDefinition.version AS version',\n",
    "                                        'mc_row.marketDefinition.name AS market_name',\n",
    "                                        'mc_row.marketDefinition.regulators AS regulators',\n",
    "                                        'mc_row.marketDefinition.runners AS runners')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_definitions = market_definitions.filter(market_definitions[\"status\"]==\"CLOSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_definitions = market_definitions.filter(market_definitions[\"market_type\"]==\"WIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_only = market_definitions.filter(market_definitions.runners.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the Runners array.\n",
    "runners_exploded = runners_only.select(market_definitions.operation_type,\n",
    "                                       market_definitions.published_time,\n",
    "                                       market_definitions.market_id,\n",
    "                                       market_definitions.market_name,\n",
    "                                       market_definitions.event_id,\n",
    "                                       market_definitions.event_name,\n",
    "                                       explode(market_definitions.runners).alias('runner_row'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the useful fields, and give them user friendly names.\n",
    "runners = runners_exploded.selectExpr('operation_type',\n",
    "                                      'published_time',\n",
    "                                      'market_id',\n",
    "                                      'market_name',\n",
    "                                      'event_id',\n",
    "                                      'event_name',\n",
    "                                      'runner_row.id AS runner_id',\n",
    "                                      'runner_row.name AS runner_name',\n",
    "                                      'runner_row.status AS runner_status',\n",
    "                                      'runner_row.sortPriority AS runner_sort_priority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to only the records that have rc (runner changes).\n",
    "rc_only = mc_exploded.filter(mc_exploded.mc_row.rc.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the rc (runner changes) array.\n",
    "rc_exploded = rc_only.select(rc_only.op.alias('operation_type'),\n",
    "                             rc_only.pt.alias('published_time'),\n",
    "                             rc_only.mc_row.id.alias('market_id'),\n",
    "                             explode(rc_only.mc_row.rc).alias('runner_change_row'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the useful fields, and give them user friendly names.\n",
    "runner_changes = rc_exploded.selectExpr('operation_type',\n",
    "                                        'published_time',\n",
    "                                        'market_id',\n",
    "                                        'runner_change_row.id AS runner_id',\n",
    "                                        'runner_change_row.ltp AS last_traded_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.researchgate.net/publication/351844751_Sonic_Thunder_vs_Brian_the_Snail_Are_people_affected_by_uninformative_racehorse_names\n",
    "fast_names = [\n",
    "    'a mile a minute', 'helter-skelter', 'quick-fire',\n",
    "    'apace', 'high-speed quickly',\n",
    "    'as fast as your legs would carry you',\n",
    "    'hot', 'quickness',\n",
    "    'as if it is going out of style', 'hotfoot', 'rapid',\n",
    "    'at a rate of knots', 'hustle', 'rapid-fire',\n",
    "    'at full pelt', 'in the twinkling of an eye',\n",
    "    'rate',\n",
    "    'at full speed', 'Jack Robinson', 'say',\n",
    "    'at full tilt', 'lick', 'shot',\n",
    "    'at full tilt', 'lickety-split', 'smartly',\n",
    "    'before you can say Jack Robinson', 'lightning', 'souped-up',\n",
    "    'blistering', 'like a shot', 'spanking',\n",
    "    'breakneck' 'like a streak of lightning', 'speed',\n",
    "    'brisk' 'like lightning spread', 'like wildfire',\n",
    "    'chop-chop', 'meteoric', 'streak',\n",
    "    'crash', 'mile', 'style',\n",
    "    'express', 'nimble', 'superfast',\n",
    "    'fast', 'nimbleness', 'supersonic',\n",
    "\n",
    "    'fleet', 'nimbly', 'swift',\n",
    "    'full', 'nippy', 'swiftly',\n",
    "    'full steam ahead', 'pdq', 'thick',\n",
    "    'gallop', 'pell-mell', 'thick and fast',\n",
    "    'galloping', 'poky', 'tilt',\n",
    "    'go like hot cakes', 'posthaste', 'top',\n",
    "    'have a heavy foot', 'precipitous', 'twinkling',\n",
    "    'headlong', 'precipitously', 'whoosh',\n",
    "    'heavy', 'prompt', 'wildfire',\n",
    "    'hell', 'promptly', 'zippy',\n",
    "    'hell for leather', 'quick',\n",
    "\n",
    "\n",
    "    'accelerated', 'high-speed', 'pell-mell',\n",
    "    'at full speed', 'hurried', 'post-haste',\n",
    "    'at full tilt', 'hurriedly', 'quick',\n",
    "    'at speed', 'in a flash', 'quickly',\n",
    "    'at the speed of light', 'in a hurry', 'rapid',\n",
    "    'blistering', 'in a trice', 'rapidly',\n",
    "    'breakneck', 'in a wink', 'smart',\n",
    "    'brisk', 'in haste', 'speedily',\n",
    "    'briskly', 'in time', 'speedy',\n",
    "    'energetically', 'in no time at all', 'sporty',\n",
    "    'expeditious', 'in the blink of an eye', 'sprightly',\n",
    "    'expeditiously', 'like a flash', 'swift',\n",
    "    'express', 'like a shot', 'swiftly',\n",
    "    'fast', 'like an arrow from a bow', 'turbo',\n",
    "    'fast-moving', 'lively', 'unhesitating',\n",
    "    'fleet-footed', 'meteoric', 'whirlwind',\n",
    "    'flying', 'nimble', 'with all haste',\n",
    "    'hastily', 'on the double', 'with dispatch',\n",
    "    'hasty', 'pell-mell', 'without delay',\n",
    "\n",
    "    'acceleration', 'haste', 'scutter',\n",
    "    'alacrity', 'hasten', 'sharpness',\n",
    "    'blast', 'hurriedness', 'shoot',\n",
    "    'bolt', 'hurry', 'spank along',\n",
    "    'bowl along', 'hurry', 'speed',\n",
    "    'briskness', 'hurtle', 'speediness',\n",
    "    'career', 'immediacy', 'sprint',\n",
    "    'celerity', 'momentum', 'stampede',\n",
    "    'charge', 'pace', 'streak',\n",
    "    'dart', 'precipitateness', 'sweep',\n",
    "    'dash', 'promptness', 'swiftness',\n",
    "    'dispatch', 'quickness', 'swoop',\n",
    "    'expedition', 'race', 'tempo',\n",
    "    'expeditiousness', 'rapidity', 'uzz',\n",
    "    'fastness', 'rate', 'velocity',\n",
    "    'flash', 'rattle along', 'whirl',\n",
    "    'fly', 'run', 'whizz',\n",
    "    'gallop', 'rush', 'whoosh',\n",
    "    'go hell for leather', 'scramble', 'wing',\n",
    "    'go like lightning', 'scud', 'zoom',\n",
    "    'hare', 'scurry',\n",
    "\n",
    "    'abrupt', 'impetuous', 'rushed',\n",
    "    'agility', 'outrun', 'scramble',\n",
    "    'dash', 'overhasty', 'speed',\n",
    "    'disconcerted', 'overrun', 'speedily',\n",
    "    'dodge', 'promptly', 'speedy',\n",
    "    'haste', 'quick', 'sudden',\n",
    "    'hastily', 'quickly', 'suddenly',\n",
    "    'hurried', 'rapid', 'swift',\n",
    "    'hurriedly', 'rapidly', 'swiftly',\n",
    "    'hurry', 'rush', 'zoom',\n",
    "\n",
    "    'accelerate', 'haste', 'race',\n",
    "    'acceleration', 'hasten', 'rapidity',\n",
    "    'agility', 'hie', 'rush',\n",
    "    'airspeed', 'hurriedly', 'speedy',\n",
    "    'celerity', 'hurry', 'stronghold',\n",
    "    'dash', 'pace', 'swift',\n",
    "    'decelerate', 'quick', 'swiftness',\n",
    "    'expedite', 'quicken', 'tempo',\n",
    "    'fast', 'quickly', 'urgently',\n",
    "    'fastness', 'quickness', 'velocity',\n",
    "\n",
    "    'Apache', 'Bentley', 'Blustery',\n",
    "    'Bullet', 'Buzz', 'Comet',\n",
    "    ',Cougar', ',Falcon', 'Faster',\n",
    "    'Flash', 'Ghost', 'rider', 'Harley',\n",
    "    'Jet', 'Jump', 'Jumping',\n",
    "    'Miles', 'Mustang', 'Pony express',\n",
    "    'Quick', 'Quicky', 'Racer',\n",
    "    'Rapid', 'Rapide', 'Rocket',\n",
    "    'Sonic', 'Speedy', 'Taz',\n",
    "    'Tornado', 'Traveler', 'Wildfire',\n",
    "    'Voyager', 'Wild', 'Velocity', \n",
    "\n",
    "    'Sonic Power', 'Speed Dragon', 'Zippy Lad', 'Lightening Vault',\n",
    "    'Powerful Jet', 'Orbit Express', 'Swift Chap', 'Blazing Tempo',\n",
    "    'Brave Falcon', 'Rush Now', 'Top Magic', 'Dixie Flyer',\n",
    "    'Esprit De Bullet', 'Strike Fast', 'Hustle Hard', 'Diamond Rush',\n",
    "    'Crown Me Fast', 'Hot Seat', 'Top Gear', 'Bright Bullet',\n",
    "    'Quick Art', 'Rush Of Blood', 'Top Boy', 'Meteoric',\n",
    "    'Moments',\n",
    "    'One Wild Guy', 'Sonic Thunder', 'Grand Gallop', 'Zippy Speed',\n",
    "    'Run for Roses', 'Saratoga',\n",
    "    'Wildcat',\n",
    "    'Quick Beers', 'Sudden Rush',\n",
    "    'Flyingwithoutwings', 'Fast On', 'Dazzlem Quick', 'You Drive I Fly',\n",
    "    'Irish Rocket', 'Hot Sauce', 'Mighty Flying', 'frost'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fast_names)):\n",
    "    fast_names[i] = fast_names[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners = runners.filter(runners.runner_status!='REMOVED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType, StringType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(BooleanType())\n",
    "def is_fast(name):\n",
    "    if name is None:\n",
    "        return False\n",
    "    name = name.lower()\n",
    "    for s in fast_names:\n",
    "        if s in name:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast = runners.withColumn('is_fast_horse',  is_fast(runners.runner_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast_only = runners_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast_only = runners_fast.filter(runners_fast.is_fast_horse=='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast_only.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast_only_win = runners_fast_only.filter(runners_fast_only.runner_status=='WINNER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast_only_win.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
