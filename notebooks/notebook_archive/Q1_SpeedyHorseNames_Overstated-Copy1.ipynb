{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.0.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/spark-3.0.0-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e2e1152b-e6b4-49fb-b8c3-73964d1bf8fb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 858ms :: artifacts dl 46ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e2e1152b-e6b4-49fb-b8c3-73964d1bf8fb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/20ms)\n",
      "21/12/08 00:48:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/08 00:48:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/12/08 00:48:50 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "21/12/08 00:48:50 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook2\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"5g\").\\\n",
    "        config(\"spark.mongodb.input.uri\",\"mongodb://mongo1:27017,mongo2:27018,mongo3:27019/database.horses_collection?replicaSet=rs0\").\\\n",
    "        config(\"spark.mongodb.output.uri\",\"mongodb://mongo1:27017,mongo2:27018,mongo3:27019/database.horses_collection?replicaSet=rs0\").\\\n",
    "        config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\").\\\n",
    "        getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/08 00:49:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "21/12/08 00:49:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "21/12/08 00:49:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "21/12/08 00:49:53 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "21/12/08 00:49:53 ERROR Utils: Uncaught exception in thread stop-spark-context\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient.stop(StandaloneAppClient.scala:283)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.org$apache$spark$scheduler$cluster$StandaloneSchedulerBackend$$stop(StandaloneSchedulerBackend.scala:232)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.stop(StandaloneSchedulerBackend.scala:129)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:724)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2135)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:1967)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1967)\n",
      "\tat org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1916)\n",
      "Caused by: org.apache.spark.SparkException: Could not find AppClient.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:169)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:548)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:552)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:74)\n",
      "\t... 9 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o50.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2188)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1157)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1151)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1220)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1196)\n\tat com.mongodb.spark.sql.MongoInferSchema$.apply(MongoInferSchema.scala:88)\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:97)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:339)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:203)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7399/2568456083.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#reading dataframes from MongoDB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mongo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mongo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o50.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2188)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1157)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1151)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1220)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1196)\n\tat com.mongodb.spark.sql.MongoInferSchema$.apply(MongoInferSchema.scala:88)\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:97)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:339)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:203)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "#reading dataframes from MongoDB\n",
    "df = spark.read.format(\"mongo\").load()\n",
    "df.createOrReplaceTempView(\"mongo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"select * from mongo array_contains(root.mc, array('CLOSED'))\").show()\n",
    "from pyspark.sql.functions import explode   # Explodes lists into rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_exploded = df.select('*', explode(df.mc).alias('mc_row'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_only = mc_exploded.filter(mc_exploded.mc_row.marketDefinition.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_only.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_exploded.head(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the marketDefinition array.\n",
    "market_definitions = md_only.selectExpr('op AS operation_type',\n",
    "                                        'clk AS sequence_token',\n",
    "                                        'pt AS published_time',\n",
    "                                        'mc_row.id AS market_id',\n",
    "                                        'mc_row.rc AS rc',\n",
    "                                        'mc_row.marketDefinition.betDelay AS bet_delay',\n",
    "                                        'mc_row.marketDefinition.bettingType AS betting_type',\n",
    "                                        'mc_row.marketDefinition.bspMarket AS bsp_market',\n",
    "                                        'mc_row.marketDefinition.bspReconciled AS bsp_reconciled',\n",
    "                                        'mc_row.marketDefinition.complete AS complete',\n",
    "                                        'mc_row.marketDefinition.countryCode AS country_code',\n",
    "                                        'mc_row.marketDefinition.crossMatching AS cross_matching',\n",
    "                                        'mc_row.marketDefinition.discountAllowed AS discount_allowed',\n",
    "                                        'mc_row.marketDefinition.eventId AS event_id',\n",
    "                                        'mc_row.marketDefinition.eventName AS event_name',\n",
    "                                        'mc_row.marketDefinition.eventTypeId AS event_type_id',\n",
    "                                        'mc_row.marketDefinition.inPlay AS in_play',\n",
    "                                        'mc_row.marketDefinition.marketBaseRate AS market_base_rate',\n",
    "                                        'mc_row.marketDefinition.marketTime AS market_time',\n",
    "                                        'mc_row.marketDefinition.marketType AS market_type',\n",
    "                                        'mc_row.marketDefinition.numberOfActiveRunners AS number_of_active_runners',\n",
    "                                        'mc_row.marketDefinition.numberOfWinners AS number_of_winners',\n",
    "                                        'mc_row.marketDefinition.openDate AS open_date',\n",
    "                                        'mc_row.marketDefinition.persistenceEnabled AS persistence_enabled',\n",
    "                                        'mc_row.marketDefinition.runnersVoidable AS runners_voidable',\n",
    "                                        'mc_row.marketDefinition.settledTime AS settled_time',\n",
    "                                        'mc_row.marketDefinition.status AS status',\n",
    "                                        'mc_row.marketDefinition.suspendTime AS suspend_time',\n",
    "                                        'mc_row.marketDefinition.timezone AS timezone',\n",
    "                                        'mc_row.marketDefinition.turnInPlayEnabled AS turn_in_play_enabled',\n",
    "                                        'mc_row.marketDefinition.version AS version',\n",
    "                                        'mc_row.marketDefinition.name AS market_name',\n",
    "                                        'mc_row.marketDefinition.regulators AS regulators',\n",
    "                                        'mc_row.marketDefinition.runners AS runners')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_definitions = market_definitions.filter(market_definitions[\"status\"]==\"CLOSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_definitions = market_definitions.filter(market_definitions[\"market_type\"]==\"WIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_only = market_definitions.filter(market_definitions.runners.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the Runners array.\n",
    "runners_exploded = runners_only.select(market_definitions.operation_type,\n",
    "                                       market_definitions.published_time,\n",
    "                                       market_definitions.market_id,\n",
    "                                       market_definitions.market_name,\n",
    "                                       market_definitions.event_id,\n",
    "                                       market_definitions.event_name,\n",
    "                                       explode(market_definitions.runners).alias('runner_row'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the useful fields, and give them user friendly names.\n",
    "runners = runners_exploded.selectExpr('operation_type',\n",
    "                                      'published_time',\n",
    "                                      'market_id',\n",
    "                                      'market_name',\n",
    "                                      'event_id',\n",
    "                                      'event_name',\n",
    "                                      'runner_row.id AS runner_id',\n",
    "                                      'runner_row.name AS runner_name',\n",
    "                                      'runner_row.status AS runner_status',\n",
    "                                      'runner_row.sortPriority AS runner_sort_priority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to only the records that have rc (runner changes).\n",
    "rc_only = mc_exploded.filter(mc_exploded.mc_row.rc.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the rc (runner changes) array.\n",
    "rc_exploded = rc_only.select(rc_only.op.alias('operation_type'),\n",
    "                             rc_only.pt.alias('published_time'),\n",
    "                             rc_only.mc_row.id.alias('market_id'),\n",
    "                             explode(rc_only.mc_row.rc).alias('runner_change_row'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the useful fields, and give them user friendly names.\n",
    "runner_changes = rc_exploded.selectExpr('operation_type',\n",
    "                                        'published_time',\n",
    "                                        'market_id',\n",
    "                                        'runner_change_row.id AS runner_id',\n",
    "                                        'runner_change_row.ltp AS last_traded_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.researchgate.net/publication/351844751_Sonic_Thunder_vs_Brian_the_Snail_Are_people_affected_by_uninformative_racehorse_names\n",
    "fast_names = [\n",
    "    'a mile a minute', 'helter-skelter', 'quick-fire',\n",
    "    'apace', 'high-speed quickly',\n",
    "    'as fast as your legs would carry you',\n",
    "    'hot', 'quickness',\n",
    "    'as if it is going out of style', 'hotfoot', 'rapid',\n",
    "    'at a rate of knots', 'hustle', 'rapid-fire',\n",
    "    'at full pelt', 'in the twinkling of an eye',\n",
    "    'rate',\n",
    "    'at full speed', 'Jack Robinson', 'say',\n",
    "    'at full tilt', 'lick', 'shot',\n",
    "    'at full tilt', 'lickety-split', 'smartly',\n",
    "    'before you can say Jack Robinson', 'lightning', 'souped-up',\n",
    "    'blistering', 'like a shot', 'spanking',\n",
    "    'breakneck' 'like a streak of lightning', 'speed',\n",
    "    'brisk' 'like lightning spread', 'like wildfire',\n",
    "    'chop-chop', 'meteoric', 'streak',\n",
    "    'crash', 'mile', 'style',\n",
    "    'express', 'nimble', 'superfast',\n",
    "    'fast', 'nimbleness', 'supersonic',\n",
    "\n",
    "    'fleet', 'nimbly', 'swift',\n",
    "    'full', 'nippy', 'swiftly',\n",
    "    'full steam ahead', 'pdq', 'thick',\n",
    "    'gallop', 'pell-mell', 'thick and fast',\n",
    "    'galloping', 'poky', 'tilt',\n",
    "    'go like hot cakes', 'posthaste', 'top',\n",
    "    'have a heavy foot', 'precipitous', 'twinkling',\n",
    "    'headlong', 'precipitously', 'whoosh',\n",
    "    'heavy', 'prompt', 'wildfire',\n",
    "    'hell', 'promptly', 'zippy',\n",
    "    'hell for leather', 'quick',\n",
    "\n",
    "\n",
    "    'accelerated', 'high-speed', 'pell-mell',\n",
    "    'at full speed', 'hurried', 'post-haste',\n",
    "    'at full tilt', 'hurriedly', 'quick',\n",
    "    'at speed', 'in a flash', 'quickly',\n",
    "    'at the speed of light', 'in a hurry', 'rapid',\n",
    "    'blistering', 'in a trice', 'rapidly',\n",
    "    'breakneck', 'in a wink', 'smart',\n",
    "    'brisk', 'in haste', 'speedily',\n",
    "    'briskly', 'in time', 'speedy',\n",
    "    'energetically', 'in no time at all', 'sporty',\n",
    "    'expeditious', 'in the blink of an eye', 'sprightly',\n",
    "    'expeditiously', 'like a flash', 'swift',\n",
    "    'express', 'like a shot', 'swiftly',\n",
    "    'fast', 'like an arrow from a bow', 'turbo',\n",
    "    'fast-moving', 'lively', 'unhesitating',\n",
    "    'fleet-footed', 'meteoric', 'whirlwind',\n",
    "    'flying', 'nimble', 'with all haste',\n",
    "    'hastily', 'on the double', 'with dispatch',\n",
    "    'hasty', 'pell-mell', 'without delay',\n",
    "\n",
    "    'acceleration', 'haste', 'scutter',\n",
    "    'alacrity', 'hasten', 'sharpness',\n",
    "    'blast', 'hurriedness', 'shoot',\n",
    "    'bolt', 'hurry', 'spank along',\n",
    "    'bowl along', 'hurry', 'speed',\n",
    "    'briskness', 'hurtle', 'speediness',\n",
    "    'career', 'immediacy', 'sprint',\n",
    "    'celerity', 'momentum', 'stampede',\n",
    "    'charge', 'pace', 'streak',\n",
    "    'dart', 'precipitateness', 'sweep',\n",
    "    'dash', 'promptness', 'swiftness',\n",
    "    'dispatch', 'quickness', 'swoop',\n",
    "    'expedition', 'race', 'tempo',\n",
    "    'expeditiousness', 'rapidity', 'uzz',\n",
    "    'fastness', 'rate', 'velocity',\n",
    "    'flash', 'rattle along', 'whirl',\n",
    "    'fly', 'run', 'whizz',\n",
    "    'gallop', 'rush', 'whoosh',\n",
    "    'go hell for leather', 'scramble', 'wing',\n",
    "    'go like lightning', 'scud', 'zoom',\n",
    "    'hare', 'scurry',\n",
    "\n",
    "    'abrupt', 'impetuous', 'rushed',\n",
    "    'agility', 'outrun', 'scramble',\n",
    "    'dash', 'overhasty', 'speed',\n",
    "    'disconcerted', 'overrun', 'speedily',\n",
    "    'dodge', 'promptly', 'speedy',\n",
    "    'haste', 'quick', 'sudden',\n",
    "    'hastily', 'quickly', 'suddenly',\n",
    "    'hurried', 'rapid', 'swift',\n",
    "    'hurriedly', 'rapidly', 'swiftly',\n",
    "    'hurry', 'rush', 'zoom',\n",
    "\n",
    "    'accelerate', 'haste', 'race',\n",
    "    'acceleration', 'hasten', 'rapidity',\n",
    "    'agility', 'hie', 'rush',\n",
    "    'airspeed', 'hurriedly', 'speedy',\n",
    "    'celerity', 'hurry', 'stronghold',\n",
    "    'dash', 'pace', 'swift',\n",
    "    'decelerate', 'quick', 'swiftness',\n",
    "    'expedite', 'quicken', 'tempo',\n",
    "    'fast', 'quickly', 'urgently',\n",
    "    'fastness', 'quickness', 'velocity',\n",
    "\n",
    "    'Apache', 'Bentley', 'Blustery',\n",
    "    'Bullet', 'Buzz', 'Comet',\n",
    "    ',Cougar', ',Falcon', 'Faster',\n",
    "    'Flash', 'Ghost', 'rider', 'Harley',\n",
    "    'Jet', 'Jump', 'Jumping',\n",
    "    'Miles', 'Mustang', 'Pony express',\n",
    "    'Quick', 'Quicky', 'Racer',\n",
    "    'Rapid', 'Rapide', 'Rocket',\n",
    "    'Sonic', 'Speedy', 'Taz',\n",
    "    'Tornado', 'Traveler', 'Wildfire',\n",
    "    'Voyager', 'Wild', 'Velocity', \n",
    "\n",
    "    'Sonic Power', 'Speed Dragon', 'Zippy Lad', 'Lightening Vault',\n",
    "    'Powerful Jet', 'Orbit Express', 'Swift Chap', 'Blazing Tempo',\n",
    "    'Brave Falcon', 'Rush Now', 'Top Magic', 'Dixie Flyer',\n",
    "    'Esprit De Bullet', 'Strike Fast', 'Hustle Hard', 'Diamond Rush',\n",
    "    'Crown Me Fast', 'Hot Seat', 'Top Gear', 'Bright Bullet',\n",
    "    'Quick Art', 'Rush Of Blood', 'Top Boy', 'Meteoric',\n",
    "    'Moments',\n",
    "    'One Wild Guy', 'Sonic Thunder', 'Grand Gallop', 'Zippy Speed',\n",
    "    'Run for Roses', 'Saratoga',\n",
    "    'Wildcat',\n",
    "    'Quick Beers', 'Sudden Rush',\n",
    "    'Flyingwithoutwings', 'Fast On', 'Dazzlem Quick', 'You Drive I Fly',\n",
    "    'Irish Rocket', 'Hot Sauce', 'Mighty Flying', 'frost'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_names_lc = (map(lambda s: s.lower(), fast_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@udf(BooleanType())\n",
    "def is_fast(name):\n",
    "    any(name.lower() in s for s in fast_names_lc)\n",
    "    \n",
    "calc_is_fast = udf(is_fast, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners = runners.withColumn('is_fast_horse',  calc_is_fast(runners.runner_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast_only = runners.filter(runners.is_fast_horse==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast_only.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/53457975/pyspark-udf-function-error-in-lambda-function\n",
    "import os\n",
    "os.environ['OBJC_DISABLE_INITIALIZE_FORK_SAFETY'] = 'YES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook2\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"5g\").\\\n",
    "        config(\"spark.mongodb.input.uri\",\"mongodb://mongo1:27017,mongo2:27018,mongo3:27019/database.horses_collection?replicaSet=rs0\").\\\n",
    "        config(\"spark.mongodb.output.uri\",\"mongodb://mongo1:27017,mongo2:27018,mongo3:27019/database.horses_collection?replicaSet=rs0\").\\\n",
    "        config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\").\\\n",
    "        getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading dataframes from MongoDB\n",
    "\n",
    "# sampleSize - https://stackoverflow.com/a/56255303\n",
    "df = spark.read.format(\"mongo\").option('sampleSize', 50000).load()\n",
    "df.createOrReplaceTempView(\"mongo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"select * from mongo array_contains(root.mc, array('CLOSED'))\").show()\n",
    "from pyspark.sql.functions import explode   # Explodes lists into rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_exploded = df.select('*', explode(df.mc).alias('mc_row'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_only = mc_exploded.filter(mc_exploded.mc_row.marketDefinition.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_only.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the marketDefinition array.\n",
    "market_definitions = md_only.selectExpr('op AS operation_type',\n",
    "                                        'clk AS sequence_token',\n",
    "                                        'pt AS published_time',\n",
    "                                        'mc_row.id AS market_id',\n",
    "                                        'mc_row.rc AS rc',\n",
    "                                        'mc_row.marketDefinition.betDelay AS bet_delay',\n",
    "                                        'mc_row.marketDefinition.bettingType AS betting_type',\n",
    "                                        'mc_row.marketDefinition.bspMarket AS bsp_market',\n",
    "                                        'mc_row.marketDefinition.bspReconciled AS bsp_reconciled',\n",
    "                                        'mc_row.marketDefinition.complete AS complete',\n",
    "                                        'mc_row.marketDefinition.countryCode AS country_code',\n",
    "                                        'mc_row.marketDefinition.crossMatching AS cross_matching',\n",
    "                                        'mc_row.marketDefinition.discountAllowed AS discount_allowed',\n",
    "                                        'mc_row.marketDefinition.eventId AS event_id',\n",
    "                                        'mc_row.marketDefinition.eventName AS event_name',\n",
    "                                        'mc_row.marketDefinition.eventTypeId AS event_type_id',\n",
    "                                        'mc_row.marketDefinition.inPlay AS in_play',\n",
    "                                        'mc_row.marketDefinition.marketBaseRate AS market_base_rate',\n",
    "                                        'mc_row.marketDefinition.marketTime AS market_time',\n",
    "                                        'mc_row.marketDefinition.marketType AS market_type',\n",
    "                                        'mc_row.marketDefinition.numberOfActiveRunners AS number_of_active_runners',\n",
    "                                        'mc_row.marketDefinition.numberOfWinners AS number_of_winners',\n",
    "                                        'mc_row.marketDefinition.openDate AS open_date',\n",
    "                                        'mc_row.marketDefinition.persistenceEnabled AS persistence_enabled',\n",
    "                                        'mc_row.marketDefinition.runnersVoidable AS runners_voidable',\n",
    "                                        'mc_row.marketDefinition.settledTime AS settled_time',\n",
    "                                        'mc_row.marketDefinition.status AS status',\n",
    "                                        'mc_row.marketDefinition.suspendTime AS suspend_time',\n",
    "                                        'mc_row.marketDefinition.timezone AS timezone',\n",
    "                                        'mc_row.marketDefinition.turnInPlayEnabled AS turn_in_play_enabled',\n",
    "                                        'mc_row.marketDefinition.version AS version',\n",
    "                                        'mc_row.marketDefinition.name AS market_name',\n",
    "                                        'mc_row.marketDefinition.regulators AS regulators',\n",
    "                                        'mc_row.marketDefinition.runners AS runners')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_definitions = market_definitions.filter(market_definitions[\"status\"]==\"CLOSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_definitions = market_definitions.filter(market_definitions[\"market_type\"]==\"WIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_only = market_definitions.filter(market_definitions.runners.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the Runners array.\n",
    "runners_exploded = runners_only.select(market_definitions.operation_type,\n",
    "                                       market_definitions.published_time,\n",
    "                                       market_definitions.market_id,\n",
    "                                       market_definitions.market_name,\n",
    "                                       market_definitions.event_id,\n",
    "                                       market_definitions.event_name,\n",
    "                                       explode(market_definitions.runners).alias('runner_row'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the useful fields, and give them user friendly names.\n",
    "runners = runners_exploded.selectExpr('operation_type',\n",
    "                                      'published_time',\n",
    "                                      'market_id',\n",
    "                                      'market_name',\n",
    "                                      'event_id',\n",
    "                                      'event_name',\n",
    "                                      'runner_row.id AS runner_id',\n",
    "                                      'runner_row.name AS runner_name',\n",
    "                                      'runner_row.status AS runner_status',\n",
    "                                      'runner_row.sortPriority AS runner_sort_priority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to only the records that have rc (runner changes).\n",
    "rc_only = mc_exploded.filter(mc_exploded.mc_row.rc.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the rc (runner changes) array.\n",
    "rc_exploded = rc_only.select(rc_only.op.alias('operation_type'),\n",
    "                             rc_only.pt.alias('published_time'),\n",
    "                             rc_only.mc_row.id.alias('market_id'),\n",
    "                             explode(rc_only.mc_row.rc).alias('runner_change_row'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the useful fields, and give them user friendly names.\n",
    "runner_changes = rc_exploded.selectExpr('operation_type',\n",
    "                                        'published_time',\n",
    "                                        'market_id',\n",
    "                                        'runner_change_row.id AS runner_id',\n",
    "                                        'runner_change_row.ltp AS last_traded_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.researchgate.net/publication/351844751_Sonic_Thunder_vs_Brian_the_Snail_Are_people_affected_by_uninformative_racehorse_names\n",
    "fast_names = [\n",
    "    'a mile a minute', 'helter-skelter', 'quick-fire',\n",
    "    'apace', 'high-speed quickly',\n",
    "    'as fast as your legs would carry you',\n",
    "    'hot', 'quickness',\n",
    "    'as if it is going out of style', 'hotfoot', 'rapid',\n",
    "    'at a rate of knots', 'hustle', 'rapid-fire',\n",
    "    'at full pelt', 'in the twinkling of an eye',\n",
    "    'rate',\n",
    "    'at full speed', 'Jack Robinson', 'say',\n",
    "    'at full tilt', 'lick', 'shot',\n",
    "    'at full tilt', 'lickety-split', 'smartly',\n",
    "    'before you can say Jack Robinson', 'lightning', 'souped-up',\n",
    "    'blistering', 'like a shot', 'spanking',\n",
    "    'breakneck' 'like a streak of lightning', 'speed',\n",
    "    'brisk' 'like lightning spread', 'like wildfire',\n",
    "    'chop-chop', 'meteoric', 'streak',\n",
    "    'crash', 'mile', 'style',\n",
    "    'express', 'nimble', 'superfast',\n",
    "    'fast', 'nimbleness', 'supersonic',\n",
    "\n",
    "    'fleet', 'nimbly', 'swift',\n",
    "    'full', 'nippy', 'swiftly',\n",
    "    'full steam ahead', 'pdq', 'thick',\n",
    "    'gallop', 'pell-mell', 'thick and fast',\n",
    "    'galloping', 'poky', 'tilt',\n",
    "    'go like hot cakes', 'posthaste', 'top',\n",
    "    'have a heavy foot', 'precipitous', 'twinkling',\n",
    "    'headlong', 'precipitously', 'whoosh',\n",
    "    'heavy', 'prompt', 'wildfire',\n",
    "    'hell', 'promptly', 'zippy',\n",
    "    'hell for leather', 'quick',\n",
    "\n",
    "\n",
    "    'accelerated', 'high-speed', 'pell-mell',\n",
    "    'at full speed', 'hurried', 'post-haste',\n",
    "    'at full tilt', 'hurriedly', 'quick',\n",
    "    'at speed', 'in a flash', 'quickly',\n",
    "    'at the speed of light', 'in a hurry', 'rapid',\n",
    "    'blistering', 'in a trice', 'rapidly',\n",
    "    'breakneck', 'in a wink', 'smart',\n",
    "    'brisk', 'in haste', 'speedily',\n",
    "    'briskly', 'in time', 'speedy',\n",
    "    'energetically', 'in no time at all', 'sporty',\n",
    "    'expeditious', 'in the blink of an eye', 'sprightly',\n",
    "    'expeditiously', 'like a flash', 'swift',\n",
    "    'express', 'like a shot', 'swiftly',\n",
    "    'fast', 'like an arrow from a bow', 'turbo',\n",
    "    'fast-moving', 'lively', 'unhesitating',\n",
    "    'fleet-footed', 'meteoric', 'whirlwind',\n",
    "    'flying', 'nimble', 'with all haste',\n",
    "    'hastily', 'on the double', 'with dispatch',\n",
    "    'hasty', 'pell-mell', 'without delay',\n",
    "\n",
    "    'acceleration', 'haste', 'scutter',\n",
    "    'alacrity', 'hasten', 'sharpness',\n",
    "    'blast', 'hurriedness', 'shoot',\n",
    "    'bolt', 'hurry', 'spank along',\n",
    "    'bowl along', 'hurry', 'speed',\n",
    "    'briskness', 'hurtle', 'speediness',\n",
    "    'career', 'immediacy', 'sprint',\n",
    "    'celerity', 'momentum', 'stampede',\n",
    "    'charge', 'pace', 'streak',\n",
    "    'dart', 'precipitateness', 'sweep',\n",
    "    'dash', 'promptness', 'swiftness',\n",
    "    'dispatch', 'quickness', 'swoop',\n",
    "    'expedition', 'race', 'tempo',\n",
    "    'expeditiousness', 'rapidity', 'uzz',\n",
    "    'fastness', 'rate', 'velocity',\n",
    "    'flash', 'rattle along', 'whirl',\n",
    "    'fly', 'run', 'whizz',\n",
    "    'gallop', 'rush', 'whoosh',\n",
    "    'go hell for leather', 'scramble', 'wing',\n",
    "    'go like lightning', 'scud', 'zoom',\n",
    "    'hare', 'scurry',\n",
    "\n",
    "    'abrupt', 'impetuous', 'rushed',\n",
    "    'agility', 'outrun', 'scramble',\n",
    "    'dash', 'overhasty', 'speed',\n",
    "    'disconcerted', 'overrun', 'speedily',\n",
    "    'dodge', 'promptly', 'speedy',\n",
    "    'haste', 'quick', 'sudden',\n",
    "    'hastily', 'quickly', 'suddenly',\n",
    "    'hurried', 'rapid', 'swift',\n",
    "    'hurriedly', 'rapidly', 'swiftly',\n",
    "    'hurry', 'rush', 'zoom',\n",
    "\n",
    "    'accelerate', 'haste', 'race',\n",
    "    'acceleration', 'hasten', 'rapidity',\n",
    "    'agility', 'hie', 'rush',\n",
    "    'airspeed', 'hurriedly', 'speedy',\n",
    "    'celerity', 'hurry', 'stronghold',\n",
    "    'dash', 'pace', 'swift',\n",
    "    'decelerate', 'quick', 'swiftness',\n",
    "    'expedite', 'quicken', 'tempo',\n",
    "    'fast', 'quickly', 'urgently',\n",
    "    'fastness', 'quickness', 'velocity',\n",
    "\n",
    "    'Apache', 'Bentley', 'Blustery',\n",
    "    'Bullet', 'Buzz', 'Comet',\n",
    "    ',Cougar', ',Falcon', 'Faster',\n",
    "    'Flash', 'Ghost', 'rider', 'Harley',\n",
    "    'Jet', 'Jump', 'Jumping',\n",
    "    'Miles', 'Mustang', 'Pony express',\n",
    "    'Quick', 'Quicky', 'Racer',\n",
    "    'Rapid', 'Rapide', 'Rocket',\n",
    "    'Sonic', 'Speedy', 'Taz',\n",
    "    'Tornado', 'Traveler', 'Wildfire',\n",
    "    'Voyager', 'Wild', 'Velocity', \n",
    "\n",
    "    'Sonic Power', 'Speed Dragon', 'Zippy Lad', 'Lightening Vault',\n",
    "    'Powerful Jet', 'Orbit Express', 'Swift Chap', 'Blazing Tempo',\n",
    "    'Brave Falcon', 'Rush Now', 'Top Magic', 'Dixie Flyer',\n",
    "    'Esprit De Bullet', 'Strike Fast', 'Hustle Hard', 'Diamond Rush',\n",
    "    'Crown Me Fast', 'Hot Seat', 'Top Gear', 'Bright Bullet',\n",
    "    'Quick Art', 'Rush Of Blood', 'Top Boy', 'Meteoric',\n",
    "    'Moments',\n",
    "    'One Wild Guy', 'Sonic Thunder', 'Grand Gallop', 'Zippy Speed',\n",
    "    'Run for Roses', 'Saratoga',\n",
    "    'Wildcat',\n",
    "    'Quick Beers', 'Sudden Rush',\n",
    "    'Flyingwithoutwings', 'Fast On', 'Dazzlem Quick', 'You Drive I Fly',\n",
    "    'Irish Rocket', 'Hot Sauce', 'Mighty Flying', 'frost'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fast_names)):\n",
    "    fast_names[i] = fast_names[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners = runners.filter(runners.runner_status!='REMOVED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType, StringType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(BooleanType())\n",
    "def is_fast(name):\n",
    "    if name is None:\n",
    "        return False\n",
    "    name = name.lower()\n",
    "    for s in fast_names:\n",
    "        if s in name:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast = runners.withColumn('is_fast_horse',  is_fast(runners.runner_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast_only = runners_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast_only = runners_fast.filter(runners_fast.is_fast_horse=='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast_only.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast_only_win = runners_fast_only.filter(runners_fast_only.runner_status=='WINNER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runners_fast_only_win.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
